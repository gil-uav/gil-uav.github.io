{
  

    "semantic-segmentation-2020-11-09-dataset-v1": {
      "title": "The dataset v0.0.1",
      "content"	 : "As stated previously in this work, the network’s goal is to segment static objects with a high amount of robustness. With this in mind, the dataset must be large and diverse enough throughout several factors such as location, lighting ,and seasonal variance. This post is a short update on the fist iteration of such a dataset.Data acquisitionThe dataset contains orthophotos covering different areas of Norway. All images came from the Norwegian Mapping Authority, Kartverket, via the University of Oslo. Pictures were taken in spatial resolution of 4 to 20 cm per pixel with three spectral bands RGB. They come from different years (2018 - 20220) and flights. The photo-flying season in Norway begins in April and lasts until the end of September. Therefore the acquired photos are characterized by a wide variety of optical conditions. They include images of different saturation, angles of sunlight, and shadows lengths. Simultaneously, the photos are from varying periods of the vegetation season. It does aid this dataset in providing robustness. However winter-landscapes and other challenging conditions are missing. To provide more diversity in the dataset, 64 map tiles were selected from various locations around all 11 different counties, spanning over 200km².No. sheetsScaleGSDHeightWidthArea heightArea lengthTotal sizeDrone FOVDrone heightNo. sets111:10004.0 cm20000 px15000 px800 m600 m9.9 Gb20 m43 m12441111:20008.0 cm20000 px15000 px1600 m1200 m9.9 Gb41 m87 m12441111:200010.0 cm16000 px12000 px1600 m1200 m6.6 Gb51 m109 m7843111:500012.5 cm25600 px19200 px3200 m2400 m11.0 Gb64 m136 m20350111:500020.0 cm16000 px12000 px3200 m2400 m6.6 Gb102 m217 m7843NB! Drone field of view (FOV) and height are approximations.ClassesAs mentioned in earlier posts, the images are annotated with two classes: water bodies and buildings. Only roads longer than 50m were available in the vector maps, so they are not usable for this work.BuildingAn object standing permanently in one place. RGB(255,255,255)Water bodiesFlowing and stagnant water, including ponds and pools. RGB(127,127,127)BackgroundAreas not classified into any class. It can include, e.g., fields, roads, and all objects excluded from above. RGB(0,0,0)AnnotationsThe ground truth labels are annotated automatically using the vector maps provided by Kartverket. However, this does provide some issues as the annotations are set per GPS points, and the orthophotos are not always perfectly rectified. Also, water bodies are not always stationary and are prone to poor annotations.1. Rectification of orthophotos was not perfect; some buildings were not completely segmented.  2. Not all buildings were segmented. 3. Water segmentation of rivers was highly inaccurate; this is due to the riverbed naturally shifting.SummaryAfter cropping sets of images and annotations from the orthophotos and annotations maps, the dataset ended up with 67219 samples at a size of 25,1Gb. However, the dataset is not yet scraped for sets not containing useful classes, erroneous annotations, or missing orthophoto coverage. This is the first Alpha version, and the next iteration will most likely be smaller and hopefully more precise.",
      "url": "/semantic_segmentation/2020/11/09/Dataset-V1/",
      "author": "Vegard Bergsvik Øvstegård",
      "categories": "semantic_segmentation"
    }
    ,
  

    "semantic-segmentation-2020-11-05-dataset-generation-pt3": {
      "title": "Dataset generation pt. 3",
      "content"	 : "In order to extract the ground truth (GT) images from the vector maps recieved by Kartverket. I used QGIS, an open source geographic information system. It was how-ever quite tedious to to extract the GT images manually using the GUI. So i wrote some simple python functions to help me automate it. To put it short, the functions import all the orthophoto maps as layers to QGIS, and exports the images with the same spatial extent said orthophotos, but from the GT vector maps.Map issuesI did discover some issues while inspecting the GT vector maps and the orthophotos:1. Rectification of orthophotos was not perfect, some buildings were not completely segmented.  2. Not all buildings where segmented. 3. Water segmentation of rivers where highly inaccurate, this is due to the riverbed naturally shifting.To solve issue 1, i used QGIS to adjust the GT vector layer a bit. I simply widened the vector by one meter at scale. The only way to solve issue number 2 and 3 is either with manual adjustment of the vector maps, or simply scrubbing the dataset after creation. Both tedious jobs, but they might be worth while. An idea to automate the scrubbing for issue number 2. is to train the network on the dataset, and then use the network identify and remove sets with non-segmented buildings. These fixes will be considered after the first training runs.QGIS helper functions:import osfrom qgis.core import QgsProjectfrom glob import globproject = QgsProject.instance()root = QgsProject.instance().layerTreeRoot()def import_map(path_to_tif: str, root):    &amp;quot;&amp;quot;&amp;quot;    Imports a .tif file as map-layer.    Parameters    ----------    path_to_tif : str        Path to .tif file.    root        Project instance layer root.    Returns    -------    &amp;quot;&amp;quot;&amp;quot;    rlayer = QgsRasterLayer(        path_to_tif, os.path.basename(path_to_tif).replace(&amp;quot;.tif&amp;quot;, &amp;quot;&amp;quot;)    )    if not rlayer.isValid():        print(&amp;quot;Layer failed to load!&amp;quot;)    iface.addRasterLayer(path_to_tif, os.path.basename(path_to_tif).replace(&amp;quot;.tif&amp;quot;, &amp;quot;&amp;quot;))def import_all_maps(path):    &amp;quot;&amp;quot;&amp;quot;    Imports all .tif files to project as layers.    Parameters    ----------    path : str        Path to folder containing .tif files(Ortophotos)    Returns    -------    &amp;quot;&amp;quot;&amp;quot;    maps = [y for x in os.walk(path) for y in glob(os.path.join(x[0], &amp;quot;*.tif&amp;quot;))]    for m in ops:        import_map(m, root)        print(&amp;quot;{} imported.&amp;quot;.format(os.path.basename(m)))def get_ortophoto_layers():    &amp;quot;&amp;quot;&amp;quot;    Returns all ortophoto layers from project.    Returns    -------    layers : list        List of ortophoto layers.    &amp;quot;&amp;quot;&amp;quot;    layers = [        l        for l in QgsProject.instance().mapLayers().values()        if l.name().startswith(&amp;quot;33&amp;quot;)    ]    layers = [        l        for l in QgsProject.instance().mapLayers().values()        if l.type() == QgsMapLayer.RasterLayer    ]    return layersdef export_basedata_as_img(layer, export_path: str):    &amp;quot;&amp;quot;&amp;quot;    Saves ground-truth for layer as .png file.    Parameters    ----------    export_path : str        Where to save the image.    layer        Orthophoto layer to produce ground-truth map from.    &amp;quot;&amp;quot;&amp;quot;    outfile = os.path.join(export_path, &amp;quot;{}_y.png&amp;quot;.format(layer.name()))    settings = QgsMapSettings()    settings.setLayers(        [            QgsProject.instance().mapLayersByName(&amp;quot;fkb_bygning_omrade&amp;quot;)[0],            QgsProject.instance().mapLayersByName(&amp;quot;fkb_vann_omrade&amp;quot;)[0],        ]    )    settings.setBackgroundColor(QColor(0, 0, 0))    settings.setOutputSize(QSize(layer.width(), layer.height()))    settings.setExtent(layer.extent())    render = QgsMapRendererParallelJob(settings)    def finished():        img = render.renderedImage()        img.save(outfile, &amp;quot;png&amp;quot;)    render.finished.connect(finished)    render.start()    print(&amp;quot;Ground truth image export of {} started.&amp;quot;.format(layer.name()))    from qgis.PyQt.QtCore import QEventLoop    loop = QEventLoop()    render.finished.connect(loop.quit)    loop.exec_()    print(&amp;quot;Ground truth image of {} exported to: {}&amp;quot;.format(layer.name(), outfile))def export_all_ground_truth_maps(export_path: str):    &amp;quot;&amp;quot;&amp;quot;    Exports ground truth images of all orthophoto layers.    export_path : str        Where to save the image.    &amp;quot;&amp;quot;&amp;quot;    for l in get_ortophoto_layers():        export_basedata_as_img(l, export_path)",
      "url": "/semantic_segmentation/2020/11/05/Dataset-Generation-Pt3/",
      "author": "Vegard Bergsvik Øvstegård",
      "categories": "semantic_segmentation"
    }
    ,
  

    "semantic-segmentation-2020-10-30-dataset-generation-pt2": {
      "title": "Dataset generation pt. 2",
      "content"	 : "As stated in the former post, the plan was to build the dataset based on an order of orthophotos with a Ground Sampling Distance(GSD) of 20cm. However, a decision was made to train the network on either 512x512 or 1024x1024 images. This to hopefully maximize efficiency and memory usage on the GPUs while training. A new order is in place, with the following specs:Order specificationsNo. sheetsScaleGSDHeightWidthArea heightArea lengthTotal sizeDrone FOVDrone heightNo. sets111:10004.0 cm20000 px15000 px800 m600 m9.9 Gb20 m43 m12441111:20008.0 cm20000 px15000 px1600 m1200 m9.9 Gb41 m87 m12441111:200010.0 cm16000 px12000 px1600 m1200 m6.6 Gb51 m109 m7843111:500012.5 cm25600 px19200 px3200 m2400 m11.0 Gb64 m136 m20350111:500020.0 cm16000 px12000 px3200 m2400 m6.6 Gb102 m217 m7843NB! Drone field of view (FOV) and height are approximations.This order will provide the dataset with a total of 60918 different sets with a height variance from about 40m to 220m. The map sheets will be extracted from different locations across each county in Norway to provide some locational variance as well. The most important aspect when choosing locations is to get as many different roof-tops and buildings as possible. Ranging from industrial, residential, and other buildings such as cabins. The latter might be the hardest for the network to segment as many cabins in Norway have turfed roofing that looks very similar to the ground below.Example of a city centerExample of turfed roofingExample industrial areaExample of a residential areaThese are the general chosen locations:Agder: KristiansandInnlandet: HamarMøre og Romsdal: ÅlesundNordland: BodøOslo: OsloRogarland: StavangerVestfold og Telemark: RjukanTroms og Finnmark: KirkenesTrøndelag: TrondheimVestland: BergenViken: Sarpsborg",
      "url": "/semantic_segmentation/2020/10/30/Dataset-Generation-Pt2/",
      "author": "Vegard Bergsvik Øvstegård",
      "categories": "semantic_segmentation"
    }
    ,
  

    "semantic-segmentation-2020-10-28-dataset-generation": {
      "title": "Dataset generation",
      "content"	 : "BackgroundIn machine learning, a data set is a collection of data. In this work, it is a collection of sets of 2 images, x, and y. Where x, the input image, is an orthophoto while the other, y, is the ground truth of buildings and other classes.Example of a set with x(left) and y(right)While there are many open-source and publicly available datasets, that contain such sets of images. It is difficult finding datasets that have the specifications needed to train such a network that this work requires. E.g lack of environmental variance, poor resolution, and different labels/ground truth.Therefore, a completely new dataset with the following requirements has to be created:Ground truth containing at least buildings.Water, roads, and other static objects are welcome.Environmental variance such as:WeatherSeasonTimeLocationAt least 1000 images per, class, weather state, season, time, and location.Good resolution and a low ground sample distance.Getting the dataAs a student at the University of Oslo, one can be provided access to some of the data not available to the public at the Norwegian Mapping Authority, Kartverket. This not only provides vectorized maps working as very good ground truths for buildings and water but also some very decent orthophotos. However, they are all taken at a time when there is little to no occlusion from leaves, snow, etc, and the timing of day to provide the best possible lighting conditions for photography. I.e lacking the environmental variance needed. The aforementioned vectorized maps did also include segmented roads, but only public roads and not drive-ways to private properties as so on. Public roads and private driveways are usually both made of asphalt, and look the same. There is a suspicion that having, say only half the valid data classified as true will cause too much noise during training. None the less a very good starting point, and with tools like QGIS, it is possible to apply orthophotos collected elsewhere and maps created using photographic mosaic and a drone.Ground sampling distance and field of viewAs the Norwegian Civil Aviation Authority states, the max flight altitude in Norway for uncertified and private drones is a maximum of 120 meters relative to the ground. Given the specifications on a typical drone such as the DJI Mavic 2 Pro, this results in a horizontal viewing field of approximately 57m by 32m on a 16:9 aspect ratio, and a Ground Sampling Distance (GSD) 2.95cm:$$ GSD_w = frac{S_w times H}{F_r times im_W} = 2.95cm $$$$begin{align*}D_w = GSD times im_W = 57mend{align*}$$ $$begin{align*}D_h = GSD times im_H = 32mend{align*}$$Dw - Horizontal plane footprint widthDh - Horizontal plane footprint heightSw = 13.2 - Sensor width(mm)H = 120 - Height(m)Fr = 28 - Focal length(mm)imW = 1920 - Image width(px)imH = 1080 - Image height(px)These specifications are somewhat important as they describe what the network will work on and most likely should train on. Ground sampling distance (GSD) is the “real-life” spatial distance between each pixel center. Some orthophotos from Kartverket can be downloaded with a GSD down to 4cm, and with a 4:3 aspect ration on full HD (1440px by 1080px), gives us a GSD of about 3.93. However, with this spatial resolution and an area of 3.2km by 2.4km, it would result in an uncompressed TIFF file of about 14Gb according to Kartverket. As the objective is to segment buildings, it seems that this high-resolution is not necessary. Then again, this work is research and such inquiries must be sought out. Below are two images comparing the GSD of 25cm and 44cm.GSD = 25cmGSD = 44cmPrevious work such as Sahu et al. [1] used GSD as high as 100cm with decent results, on a very similar task as this. On the other hand Horwath et al [2]. mentions improvements in accuracy for high resolution images in electron microscopy images, but not without challenges. The aforementioned task is although not directly transferable to segmenting buildings, as they were looking for very small particles with a radius of circa 4-6 pixels in a 1024x1024 image. E.g a small shed with a size of 1m2 would take up about 20x20 pixels, and most static buildings are bigger than 1m2.Another point is that the network will run on a drone with limited computational resources. Floating-point operations are not free and unlimited. For this reason images with a GSD of 20 will suffice as a start, and if need be, access to higher resolution images is possible. As previously mentioned, images captured with a drone might also be added due to the lacking environmental variance that the images from Kartverket have. These will have almost the same GSD calculated above as the drone that will be used is a DJI Mavic Pro.50 Gb of raster maps have been ordered, and the dataset will be adapted and updated continuously until adequate results on real drone photos have been produced by a potential network. With this in mind, the test-set will mostly contain drone footage as this is what the network will process when the framework is in production. The first version of the dataset is hopefully finished by the end of next week. The complete version, however, will not be ready until enough environmental variance is captured, and to no one’s surprise, time nor weather can be controlled.[1] M. Sahu and A. Ohri, “VECTOR MAP GENERATION FROM AERIAL IMAGERY USING DEEP LEARNING,” ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences, vols. IV-2/W5, no. 2/W5, pp. 157–162, May 2019, doi: 10.5194/isprs-annals-IV-2-W5-157-2019.[2] J. P. Horwath, D. N. Zakharov, R. Mégret, and E. A. Stach, “Understanding important features of deep learning models for segmentation of high-resolution transmission electron microscopy images,” npj Computational Materials, vol. 6, no. 1, Dec. 2020, doi: 10.1038/s41524-020-00363-x.",
      "url": "/semantic_segmentation/2020/10/28/Dataset-Generation/",
      "author": "Vegard Bergsvik Øvstegård",
      "categories": "semantic_segmentation"
    }
    ,
  

    "semantic-segmentation-2020-10-25-semantic-segmentation-background": {
      "title": "Semantic Segmentation Background",
      "content"	 : "Semantic SegmentationExample of Semantic images segmentation where buildings have been segmented(white).The goal of semantic image segmentation is to classify each pixel of an input image with a corresponding class of what is being represented. Because we’re predicting for every pixel in the image, this task is commonly referred to as dense prediction. The expected output in semantic segmentation is a complete high resolution image in which all the pixels are classified.Convolutional Neural NetworksA Convolutional Neural Network (CNN), in short, is a deep learning algorithm commonly used to, assign importance and differentiate between various objects and aspects of an image fed into it. It does so by changing and updating inherent weights and biases, based on a ground truth via supervised learning.To some extent, they are very similar to regular neural networks that use Multilayer perceptrons(MLPs), both are made up of learnable weights. But contrary to MLPs, CNNs have an architecture that explicitly assumes its inputs have structures like images. This allows encoding said property into the architecture, by sharing the weights for each location in the image and having neurons respond only locally. I.e a CNN is composed of convolutional layers without any fully-connected layers or MLPs usually found at the end. This provides efficiency for the forward pass implementation, and most importantly reduces the number of parameters in the network compared to a fully connected network(FCN). E.g if a 3-channel image of size 256 by 256 pixels were to be feed into an FCN, it would require the first hidden layer to have 196608 input weights.A common outline of a very shallow Convolutional Neural Network. One Convolutional layer and one Pooling layer. Input images are depicted as a 3D block as they often have 3 channels(RGB).The pre-processing required in a CNN is also much lower as compared to other classification algorithms. While in primitive methods filters are hand-engineered, with enough training, CNNs can learn these filters. The architecture of a CNN is analogous to that of the connectivity pattern of Neurons in the Human Brain and was inspired by the organization of the Visual Cortex. Individual neurons respond to stimuli only in a restricted region of the visual field known as the Receptive Field. A collection of such fields overlap to cover the entire visual area. In particular, a node in a layer is only connected to a small region of the previous layer, contrary to each node in the layer before in a FCN. As mentioned, this reduces the number of parameters vastly.Forward propagationForward propagation in a CNN architecture consists mostly of convolutional layers, pooling layers, and activation functions.Convolution is the first layer to extract features from an input image or a previous layer. Convolution preserves the relationship between pixels by learning image features using small filters, also known as kernels, passing them over the image. I.e layer values from small areas in the previous layer or input images are used to calculate new values in the current layer. The small area is covered by a kernel of trainable weights, and the use of a compact area allows the new value in the current layer to retain surrounding information from that area in the former layer.Pooling layers offer an approach to downsample areas, known as feature maps, by summarizing the presence of the different values in patches of the feature map. Two common pooling methods are average pooling and max pooling that summarize the average presence of a feature and the most activated presence of a feature respectively. In short, it is an operation whose objective is to reduce spatial size of the input by selecting a certain value and discarding the rest.Activation functions are usually somewhat simple functions implemented at the end of a layer. They are highly important as they introduce non-linearity to the networks, which allow the layer and neurons to learn and pass answers down the pipeline.Commonly, the last layers of CNNs are fully connected layers to make predictions. However, in this work, other modules are used in the aft parts of the architecture. For this reason, there are no further mentions of FCNs. The output layer is generally a Softmax layer to clamp the class scores to a value between 0 and 1. The aforementioned layers are described in this subsection.Convolutional layerConvolutional layers are the most important building blocks used in CNNs, hence their name[1]. In the context of a CNN, convolution is a linear operation that involves the multiplication of a set of weights with the input, much like a traditional neural network. Given that the technique was designed for two-dimensional input, the multiplication is performed between an array of input data and a two-dimensional array of weights, called a filter or a kernel.The outline of a convolutional layer.The filter is smaller than the input data, also the type of multiplication applied between a filter-sized patch of the input and the filter is a dot product. This multiplication produces a single scalar, resulting in a two-dimensional activation map. The spatial size of this activation map depends on whether the input was padded or not. Padding refers to the number of pixels added to an image when it is being processed by the kernel. Adding padding to an image processed by a CNN allows for a more accurate analysis of images, and allow the spatial size of the resulting activation map to be the same as the input.Pooling LayerPooling is a common feature imbibed into CNN architectures, and the main idea behind these layers is to sum-up features from maps generated by previous convolution layers[1]. Formally its function is to reduce the spatial size of the activation maps, such that it reduces the number of parameters and hence computation in the network.The most common form is max pooling. In a relatively simple operation, a kernel of the chosen size is applied as a sliding window across each activation map individually. The largest value within the area the kernel is applied to in the activation map is then chosen as the representative for that area. The kernel size and the stride are hyperparameters chosen by the designer of the architecture. The stride tells of how many pixels the kernel jump after each application. If the stride is equal to the number of rows in the kernel, none of the areas the kernel is applied to will overlap. In a max-pool layer, this is the most common approach.Example of max-pooling in a single channel activation map with kernel sizes of 2x2, and stride of 2.Max pooling is done in part to help over-fitting by providing an abstracted form of the activation maps. As well as it reduces the computational cost by reducing the number of parameters to learn, and it also provides basic translation invariance to the internal representations. Max pooling is done by applying a max filter over non-overlapping subregions of the initial activation maps.The idea is to retain the information which best describes the context of the image, from each region and throw away the information which is not important.Rectified Linear UnitThe Rectified Linear Unit(ReLu) is a non-linear activation function that is used in multi-layer neural networks or deep neural networks. For input values of x the function can be represented as:$$begin{align*}f(x) = max(0,x)end{align*}$$(1) According to equation 1, the output of ReLu is the maximum value between zero and the input value. The output is equal to zero when the input value is negative and the input value when the input is positive. I.e$$begin{align*}f(x) = begin{cases}x, mbox{if } x geq 00, mbox{otherwise}end{cases}end{align*}$$(2) Traditionally, some prevalent non-linear activation functions, like sigmoid functions and hyperbolic tangent, have been used in neural networks to get activation values for each neuron. However, the ReLu function has become a more popular activation function because it can accelerate the training speed of deep neural networks compared to traditional activation functions. This because the derivative of ReLu is 1 for positive input. Owing to a constant, deep neural networks do not need to take additional time for computing error terms during the training phase.The ReLu function does not trigger the vanishing gradient problem when the number of layers grows. This is because the function does not have an asymptotic upper and lower bound. Thus, the earliest layer (the first hidden layer) can receive the errors coming from the last layers to adjust all weights between layers. By contrast, a traditional activation function like sigmoid is restricted between 0 and 1, so the errors become small for the first hidden layer. The mentioned scenario will lead to a poorly trained neural network.Normalization LayerOne of the most common Normalization techniques used nowadays is Batch Normalization (BN). It is a strategy that normalizes interlayer outputs in a neural network. This in effect resets the distribution of the output from previous layers to be more efficiently processed by the subsequent layers[2].It relieves numerous problems with properly initializing neural networks. In practice, networks that use BN are significantly more robust to bad initialization. Additionally, BN can be interpreted as doing preprocessing at every layer of the network, but integrated into the network itself in a differentiable manner[3].The method leads to faster learning rates, as normalization ensures that there are no extreme activation values. It also allows each layer to learn independently from others, and it reduces the amount of data lost between the processing layer. This improves learning accuracy throughout the network. Ioeffe and Szegezy [4] report a state of the art classification model that achieved the same accuracy but requiring 14 times fewer learning iterations to do so using BN.However, according to Wu et al. [5] using batch sizes (BS) less than 32 with BN results in a dramatically increased model error. There are situations that one has to settle for sizes of BS less than 32. I.e when memory consumption of each data sample is too high, with large networks or simply with lacking hardware requirements. This work does handle somewhat high-resolution images, and for this reason, alternatives to BN which work well with small batch size are needed. Group Normalization (GN), proposed by Wu et al. is one of the latest normalization methods that avoids exploiting the batch dimension, thus is independent of batch size. They report that with a ResNet-50 model trained in the ImageNet training set using 8 GPUs, a reduction in BS from 32 to 2 resulted in little to no change in error with GN contrary to an increase in error when using BN.ImageNet classification error vs. batch sizes. [5] Softmax layerIn many neural networks, the last layer is often a softmax layer[2]. It is used to transform values of the class scores to numbers ranging from 0 to 1. The sum of the class-wise predictions is 1, so the layer can be interpreted as a probability spread function and looks like this: $$f_j(mathbf{z}) = frac{e^{z_j}}{sum_ke^{z_k}}$$(3)  where z is the set of scores to be squashed. As one can see, the formula takes each class score in the power of e, and divides it by the sum of the entire set in the power of e.BackpropagationBackpropagation, short for backward propagation of errors is arguably the most important part of the training process. This is where the learnable parameters, weights, and biases in the network are updated to make improved predictions [6]. When a network is run through the training loop, a loss function is calculated, which represents the network’s predictions and its distance from the true labels. Backpropagation allows us to calculate the gradient of the loss function, proceeding backward throughout the network from the last layer to the first. For the gradient to be calculated at a particular layer, the gradients of all following layers are combined via the chain rule of calculus. This enables each weight to be updated individually to, gradually reduce the loss function over many training iterations. Loss functions are described in more detail further down in this section.The loss function provides the loss, or error, L at each output node, and the objective is to minimize this value. The input to this function is x and consists of the input values in the training data and the learnable parameters in the network. The input data is static and cannot be altered to minimize L, but the parameters are. The backpropagation method calculates how much the parameters should be altered, by finding their gradients relative to the error output from the loss function. Said gradient is calculated by taking the partial derivatives of the output with respect to input values. E.g if an output depends on three input values, the output has three partial derivatives. The gradient itself is the vector consisting of all these partial derivative values.To attain the gradient, partial derivatives between each parameter relative to the error output that the particular parameter contributed to must be calculated. For this to be possible, the process must start at the output layer. Every node obtains input values from a set of nodes in the previous layer. In the forward pass, each node calculates its output values based on the input and the gradient of the input values relative to its output value. During backpropagation, moving backward from the output layer to the input layer. All the nodes in time learn the gradient of its output value relative relative to output of the value it contributed to calculating. Per the chain rule, the aforementioned gradient should then be multiplied with all the local gradients that the node has obtained. The process hence repeats when the input nodes to the particular node know the gradient of its output relative to the final error output value. In this manner, the gradient for every parameter from the output to the input layer will know its gradient relative to the final error output value of which they contributed to, and can be modified to minimize said error based on the gradient value.Loss functionsThe loss function is used to determine the error or in other words the loss, between a network prediction and a given target value. It expresses how far off the network is at doing a correct prediction. It is usually expressed as a scalar that increases by how far off the model is. As the goal of the model is to perform correct predictions, the main objective of the training process is to minimize this error.A common loss function is the cross-entropy loss function. Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. The function can be described as:$$ -sum^M_{c=1}y_{o,c}log(p_{o,c}) $$(4) where M is the number of classes, y is the binary indicator if class label c is the correct prediction for the observation o and p is the predicted probability observation o With binary classification, where the number of classes M equals 2, the cross-entropy can be calculated as:$$begin{align*}-(ylog(p)+1-y)log(1-p))end{align*}$$(5) If M &amp;gt; 2 we calculate a separate loss for each class label per observation and sum the result.In practice, this gives a high loss value to wrong predictions and a 0 loss value to the right predictions. This is the behavior that is wanted in a loss function since when minimized it will give better predictions. This is the base of the loss function used in this work.Fully Convolutional NetworkThe goal of semantic image segmentation is to classify each pixel of an input image with a corresponding class of what is being represented. Because we’re predicting for every pixel in the image, this task is commonly referred to as dense prediction. The expected output in semantic segmentation is a complete high-resolution image in which all the pixels are classified.In a typical convolutional network, the height and width of the input gradually reduce i.e downsampling, because of pooling. This helps the filters in the deeper layers to focus on a larger receptive field. However the depth, number of filters used, gradually increase which aids in extracting more complex features from the image. From the pooling layers, one can somewhat conclude that by downsampling, the model better understands the context presented in the image, but it loses the information of locality, i.e where said context is located. Thus if one were to use a regular convolutional network with pooling layers and dense layers, the locality information will be lost and one only retains the contextual information.A Fully Convolutional Network(FCN) is a CNNs based network that progresses from coarse to fine inference to predict every pixel like semantic segmentation requires[7].DeconvolutionTo get an output that is expected in semantic segmentation, there is a need to convert, or upsample, the low-resolution information provided by a typical CNN to high resolution, and recover the locality information.Deconvolution, sometimes also called Transposed Convolution or fractionally strided convolution, is such a technique to perform upsampling of an image with learnable parameters[8]. On a high level, transposed convolution is exactly the opposite process of a normal convolution i.e., the input volume is a low resolution image and the output volume is a high-resolution image. A normal convolution can be described as a matrix multiplication of input image and filter to produce the output image. In short, by taking the transpose of the filter matrix, it is possible to reverse the convolution process, hence the name transposed convolution.[1] “CS231n Convolutional Neural Networks for Visual Recognition.” Accessed: Oct. 24, 2020. [Online]. Available: https://cs231n.github.io/convolutional-networks/.[2] “Batch Normalization Definition | DeepAI.” Accessed: Oct. 23, 2020. [Online]. Available: https://deepai.org/machine-learning-glossary-and-terms/batch-normalization.[3] “CS231n Convolutional Neural Networks for Visual Recognition.” Accessed: Oct. 23, 2020. [Online]. Available: https://cs231n.github.io/.[4] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network training by reducing internal covariate shift,” in 32nd international conference on machine learning, icml 2015, Feb. 2015, vol. 1, pp. 448–456, [Online]. Available: https://arxiv.org/abs/1502.03167v3.[5] Y. Wu and K. He, “Group Normalization,” International Journal of Computer Vision, vol. 128, no. 3, pp. 742–755, Mar. 2020, doi: 10.1007/s11263-019-01198-w.[6] “Backpropagation Definition | DeepAI.” Accessed: Oct. 25, 2020. [Online]. Available: https://deepai.org/machine-learning-glossary-and-terms/backpropagation.[7] J. Long, E. Shelhamer, and T. Darrell, “Fully Convolutional Networks for Semantic Segmentation,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 39, no. 4, pp. 640–651, Nov. 2014, [Online]. Available: http://arxiv.org/abs/1411.4038.[8] “Fractionally-Strided-Convolution Definition | DeepAI.” Accessed: Oct. 25, 2020. [Online]. Available: https://deepai.org/machine-learning-glossary-and-terms/fractionally-strided-convolution.",
      "url": "/semantic_segmentation/2020/10/25/Semantic-Segmentation-Background/",
      "author": "Vegard Bergsvik Øvstegård",
      "categories": "semantic_segmentation"
    }
    ,
  

    "2020-10-16-second-presentation": {
      "title": "Second progress presentation",
      "content"	 : "Second progress presentationVegard Bergsvik ØvstegårdFri - 16 Oct 2020 Days until delivery: 214 daysGIL-UAVGPS-Independent Localization for UAVsSystem diagramFramework simulationStatus from previous progress presentationTaskProgressIn progress:1. Create U-net (PyTorch, multi-GPU)2. Acquire &amp;amp; improve DatasetTo do:3. Train the U-net4. Code dataset-producing software5. Get drone footage6. Implement framework(C++, SIMD, CUDA)CompletedImplement naive MCL algorithm (Python)Get hardware (nVIDIA Jetson TX1)Updated tasksTaskProgress1. Create U-net (PyTorch, multi-GPU)2. Acquire &amp;amp; improve Dataset3. Train the U-netCreate U-net (PyTorch, multi-GPU) Base model and framework implemented with PyTorch and PyTorch IgniteFeatures:Gradient clippingGradient accumulationEarly stoppingMetricsDistributed data parallel(DDP)Multi GPU trainingTraining notification(Discord)Learning rate scheduler (ReduceLROnPlateau)Rescaling of images.Create U-net (PyTorch, multi-GPU) Features:Data augmentation for training(All are random to some degree):ContrastBrightnessSaturationNoise; Gaussian, Salt &amp;amp; Pepper, Poission, Speckle.Rotations($$n*frac{pi}{2}$$)Vertical and Horisontal flips.Acquire &amp;amp; improve Dataset Extracted larger aerial photographs from existing data, of which will aid in expanding the dataset by several images.Fine-tuned the ground truth segmentation images as they where not overlapping buildings enough.Have finally gotten access to Kartverkets database: Acquire &amp;amp; improve Dataset Resultsnotes:Ground truth is far from perfect even after fine-tuning. This does yield noise in the dataset, witch may lead to the following cases:NEGATIVE: Learning gets corrupted and a suitable global optimum may never occur.POSITIVE: Noise may induce better generalization for the network.Real life scenarios contain occlusions in the images, and network may learn to draw buildings as squares despite say occluding trees.Train the U-net Attained several good preliminary results despite the network not being tuned and with a lacking dataset.Somewhat sceptical to the results. Test and validation data is completely separate, but they are very similar. I assume images from example drone-footage will not be as good.Results look like overfitting, but no indications of it from metrics.Train the U-net  Segmented buildings masked in white to the right.Train the U-net  Ground truth left and prediction right.Train the U-net Training rounds left, validation rounds right.  No indications of overfitting. Light blue had learning-rate of 0.001. Metric is Binary Cross Entropy with Logits(BCEWithLogitsLoss).Train the U-net Resultsnotes:Results appear to be overfitting, however without indications in metrics.Image resolution plays a big part, 512x512px images gave far better results then 256x256px.I am considering increasing image size further, how ever this will affect batch-size and memory usage on GPUs.To much variation and randomness in data augmentation is disruptive for training. I.e to much noise results in no training(Duh).Considering adding elastic transformations to data-augmentation as not all buildings are square.Also considering adding grayscale transformations as the network appears to find red-bricked roof-tops much faster than any other. This might aid in a better global optimum.Current status and progressTaskProgressIn progress:1. Tune the U-net**2. Acquire &amp;amp; improve Dataset*3. Train the U-net*To do:4. Code dataset-producing software5. Get drone footage6. Implement framework(C++, SIMD, CUDA)CompletedCreate U-net (PyTorch, multi-GPU)*Updated tasks* New tasks**Completed tasksCreate U-net (PyTorch, multi-GPU)Implement naive MCL algorithm (Python)Get hardware (nVIDIA Jetson TX1)Plan for the next fortnight: Week 43Week 44Tune and train the U-netTune and train the U-netAcquire &amp;amp; improve DatasetGet drone footageUpdate master thesis with current results and findingsCreate orthophoto maps from drone-footagePrintBack",
      "url": "/2020/10/16/Second-Presentation/",
      "author": "Vegard Bergsvik Øvstegård",
      "categories": ""
    }
    ,
  

    "semantic-segmentation-2020-09-23-pytorch-optimizations": {
      "title": "Pytorch(v&gt;=1.6.0) performance tuning tips",
      "content"	 : "Enable asynchronous data loading &amp;amp; augmentationPyTorch DataLoader supports asynchronous data loading/augmentation. The defaults settings are with 0 threads and no pinned memory. Use num_workers &amp;gt; 0 to enable asynchronous data processing, and it’s almost always better to use pinned memory.Default settings:DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,           batch_sampler=None, num_workers=0, collate_fn=None,           pin_memory=False, drop_last=False, timeout=0,       worker_init_fn=None)This is faster.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,           batch_sampler=None, num_workers=8, collate_fn=None,           pin_memory=True, drop_last=False, timeout=0,       worker_init_fn=None)You know how sometimes your GPU memory shows that it’s full but you’re pretty sure that your model isn’t using that much? That overhead is called pinned memory. ie: this memory has been reserved as a type of “working allocation.” When you enable pinned_memory in a DataLoader it automatically puts the fetched data Tensors in pinned memory, and enables faster data transfer to CUDA-enabled GPUs.Pinned memoryEnable cuDNN autotunerFor convolutional neural networks, enable cuDNN autotuner by setting:torch.backends.cudnn.benchmark = TruecuDNN supports many algorithms to compute convolution, and the autotuner runs a short benchmark and selects the algorithm with the best performance.Increase batch sizeOften AMP reduces memory requirements due to half float precision utilization. Thus increase the batch size to max out GPU memory.When increasing batch size: * Tune the learning rate * Add learning rate warmup and learning rate decay * Tune weight decay or switch to optimizer designed for large-batch training * LARS * LAMB * NVLAMB * NovoGradDisable bias for convolutions directly followed by a batch norm.Disable biasUse parameter.grad = None instead of model.zero_grad()Not this:model.zero_grad()But this!for param in model.parameters():    param.grad = NoneThe former executes memset for every parameter in the model, and backward pass updates gradients with “+=” operator (read + write). This is a slow and naive implementation of PyTorch, will hopefully be fixed. The latter doesn’t execute memset for every parameter, memory is zeroed-out by the allocator in a more efficient way and backward pass updates gradients with “=” operator (write).Disable debug APIS for final trainingThere are many debug APIs that might be enabled, this slows everything down. Here are some: * torch.autograd.detect_anomaly * torch.autograd.set_detect_anomaly(True) * torch.autograd.profiler.profile * torch.autograd.profiler.emit_nvtx * torch.autograd.gradcheck * torch.autograd.gradgradcheckUse efficient multi-GPU backendDataParalell uses 1 CPU core and 1 python process to drives multiple GPUs. Works for a single node, but even for that DistributedDataparallel is often faster. IT provides 1 CPU core for each GPU, likewise 1 python process for each GPU. It can do Single-node and multi-node (same API), has efficient implementation with automatic bucketing for grad all-reduce, all-reduce overlapped with backward pass and is for all intended purposes multi-process programming.Fuse pointwise operationsPyTorch JIT can fuse pointwise operations into a single CUDA kernel. Unfused pointwise operations are memory-bound, for each unfused op PyTorch has to: * launch a separate CUDA kernel * load data from global memory * perform computation * store results back into global memoryI.e from this:def gelu(x):    return x * 0.5 * (1.0 + torch.erf(x / 1.41421))to this:@torch.jit.scriptdef fused_gelu(x):    return x * 0.5 * (1.0 + torch.erf(x / 1.41421))Construct tensors directly on GPUsDont do this:t = tensor.rand(2,2).cuda()Instead, create the tensor directly on the device:t = tensor.rand(2,2, device=torch.device(&amp;#39;cuda:0&amp;#39;))Its faster!Summary:use async data loading / augmentationenable cuDNN autotunerincrease the batch size, disable debug APIs and remove unnecessary computationefficiently zero-out gradientsuse DistributedDataParallel instead of DataParallelapply PyTorch JIT to fuse pointwise operations",
      "url": "/semantic_segmentation/2020/09/23/Pytorch-Optimizations/",
      "author": "Vegard Bergsvik Øvstegård",
      "categories": "semantic_segmentation"
    }
    ,
  

    "2020-09-22-essay": {
      "title": "GPS-independent localization framework for Aerial vehicles",
      "content"	 : "IntroductionIn recent years, the popularity and use of Unmanned Aerial Vehicles (UAV) have increased drastically [1]. The usage and potential for both civilian and military applications are plural. From crop dusting and monitoring, infrastructure inspections to surveillance, and accident reporting. Drones have an increasingly larger presence and influence on our lives. With this in mind, it is very important that they and the systems they depend on work as intended.Most classes of robots, in civilian and military applications, localization and navigation are fundamental capabilities. They are especially important for UAVs to execute complex operations. Inertial Navigation System (INS) and Global Positioning Systems (GPS) are often an integrated and crucial part of a UAV’s navigation systems, despite their weaknesses.State estimation from INS is rarely used alone and mostly aids position estimation when the GPS signal is unavailable or corrupt. However, due to hardware imperfections [2] that are very difficult to avoid, INS will drift over time.Carrol [3] and Caballero [4] et al. state that the number of satellites and the quality of their respective signals play an important part in estimating a GPS receivers position. Few satellites or degraded signals will affect the estimation. Many factors affect GPS signals and estimation [5], those most relevant to Aerial Vehicles (AV) are for instance Signal Occlusion (SO). This happens when satellite signals are blocked due to buildings, bridges, trees, or other obstacles. When signals are reflected off buildings or walls, causing multipath propagation (MP). Jamming, Radio interference, or some Atmospheric conditions such as major solar storms can cause issues and corrupt signals. Satellite maintenance or maneuvers creating temporary gaps in coverage are also problematic.As mentioned, UAVs are heavily dependent on GPS for navigation and localization. To obtain a robust positioning system, the many potential errors related to GPS has to be overcome. In recent years, active jamming of GPS signals has been claimed by the Norwegian government [6], resulting in a disruption of civilian flights. I.e having a GPS independent localization solution is of high importance and in some situations crucial. This motivates for developing alternate localization methods, able to work both alongside and independent of existing ones. Weight, space, size, and battery capacity are limited resources on UAV’s, i.e there must be a cap on said factors and the upsides of the functionality must outweigh the downsides of the added hardware.In addition to GPS and INS, cameras are a very common sensor embedded on UAV platforms. With the continuing reduction in weight, price, and size compared to LIDAR or Laser Range Finder, the use of cameras has increased and is often regarded as a standard sensor. As images contain massive amounts of information about the environment, they are useful for many tasks. Vision-based localization systems are one of them and use only one or several embedded cameras on the UAV and a map of the environment. They do not rely on other external systems such as ground stations or satellites. Thus, a camera serves as a good candidate for a redundant localization solution or replacement when GPS fails.Mantelli et al. [7] mention a possible approach and its challenges for the vision-based UAV localization problem. Using a downwards facing camera, providing aerial images of the environment, and estimating the position of the images on an a priori known map that is based on aerial or satellite images. Most of the planet is already mapped using the aforementioned methods, there are many free and online sources providing maps like these such as Google™ Earth, Bing ™ Maps, and others. There are also increasingly more areas with detailed topography information from LIDARS and other sensors. Some of the quoted challenges with this problem are the update frequency and resolution of the maps. The images collected by the UAV might also have a significant difference compared to the a priori map. Illumination conditions, transient ground modifications caused by moving objects, weather conditions in particular rainfall and snow, but also long-term static modifications such as new roads or buildings.Many works propose different types of maps and measurement models to overcome the challenges related to the UAV localization problem. Concerning this problem; maps are often 2D orthogonal maps or 3D point-clouds of the environment, and measurement models are functions that compute the similarity of the UAVs sensory data and a patch of the map. They do however come with caveats, some only work in specific scenarios where robustness falls out, and others have high computational cost or lacking precision. Hence a novel approach to improve such solutions is important to localize the UAV with high robustness and low power usage.This essay proposes a somewhat novel strategy but is inspired by Mantelli et al. [7], Masselli et al. [8] and Nassar et al. [9]. A downward-facing camera and a vision-based measurement model are used, and an extra step is added in an attempt to improve robustness and decrease the computational cost. The idea is to include an image segmentation network such as U-net [10], and use a very simple binary image descriptor on the segmented images from the camera and the a priori map. Robustness is induced by training the network to be invariant to some of the aforementioned challenges. E.g it should segment out stationary objects such as buildings despite some illumination conditions, weather, and seasonal changes such as snow. To estimate the UAV pose in 4 degrees of freedoms(DoF), the vision-based localization framework will apply the measurement model in a particle filter approach such as Monte Carlo Localization(MCL) [11]. In short, a segmented image of the UAV’s view is compared to several random patches on the a priori segmented map, and the measurement model describes their similarity. With enough particles over time, the framework should find one that is similar enough to provide a likely position.Global localization is made with comparing the segmented image captured by the UAV, c), with the a priori map representing the ground truth b), describing the environment a) with segmenting long-term objects such as buildings and roads. As per , the suggested framework compares the image observed and segmented by the robot, c), with a patch from the ground truth, b). As we are passing the observed image through a network, there might be a need to pre-process the observed images. However, as the segmentation reduces the dimensionality of the images, the computational cost of comparing the observed image to a patch is lower. E.g the measurement model compares a few classes and not the different pixel values. This in turn resulting in higher robustness and hopefully lower computational cost. The approach is proposed as a redundant framework to estimate the UAV’s pose. There are nevertheless some situations where the framework will have trouble producing a precise position as when passing over areas with little to no buildings or roads. There might also be errors when flying over homogeneous regions.Prior simulations have shown that a simple binary measurement model used in an MCL approach is highly successful in localizing the UAV’s pose, but this is given a perfect segmentation. However, with the stochastic nature of the MCL approach, a segmentation network of mediocre quality in accuracy and precision with some post-processing might produce decent results in localizing the UAV. Morphological operations such as dilation and erosion combined with shape matching using Hu moments [12] might improve localization as buildings more often than none have somewhat strict geometric shapes. I.e creating squares and rectangles out of the blobs that the network might produce. Nonetheless, the simulation is an indicator that the suggested framework does bear some merit.Background and Related workIn recent years, advances in non-GPS localization of UAVs have been made, showing that this method has promise. While their implementation differs, MCL is often used. Many works propose vision-based solutions to the UAV localization problem using different approaches. They often deal with two different fundamental tasks, global localization and position tracking. Furthermore, we show some works that use different descriptors for image matching in vision-based solutions, and also different approaches to image segmentation. Both global localization, and position tracking, describe the problem of determining a robot’s pose relative to a given map of the environment. However, the latter knows the robots initial pose, while the former does now.Global localization:  Masselli et al. [8] attempt UAV localization with a particle filter and terrain classification through feature extraction. Their solution provides global localization and an average error of 5.2m but is not proven to be robust against all environmental changes, just some lighting, and seasonal changes. We believe that segmentation through Deep Learning will yield a much more accurate result that is more robust against environmental changes.  Mantelli et al. [7] propose a new localization strategy for a UAV equipped with a downward-facing camera, using a robust vision-based measurement model. The proposed measurement model computes the likelihood of the robot pose with the aid of an improved descriptor called abBRIEF [7], based on BRIEF [13]. The abBRIEF descriptor differs from BRIEF in two points: the color space used and the noise image reduction strategy. Their vision-based localization system applies the new measurement model in a Monte Carlo Localization (MCL) approach [11] that estimates the UAV pose in 4 degrees of freedom (DoF). In this paper the UAV is located within a short period, outperforming previous measurement models and yielding low errors, but is not proven to be robust against environmental changes like lighting and seasonal changes. We build upon this approach using a much simpler binary descriptor in combination with image segmentation.  Viswanathan et al. [14] demonstrate a working implementation of semantic segmentation with a Bayesian localization algorithm for ground vehicles across seasons, successfully localizing in satellite maps from summer, winter, and spring. Inherently, solving the localization problem is much harder for Unmanned Ground Vehicle (UGV) than for UAV, due to the drastic shift in perspective from the ground images to satellite map images. Although this paper also uses segmentation with LIDAR to locate roads. It gives merit to that invariance across seasons can be solved when using semantic segmentation and a particle filter. An important note is that their “winter” environment contained no snow, but this can be included when training the network.Position tracking:  Nassar et al. [9] showing successful segmentation of satellite imagery using U-Net, but using a custom Semantic Shape Matching algorithm to establish the location in the satellite map. While the segmentation is largely successful, localization is sub-par and robustness against environmental changes is not proven. This framework also uses SIFT [15] Registration making the framework more computational heavy, and it does not inherently provide global localization.  Surber et al. [16] also presented an approach to localize a UAV locally, using the UAV’s onboard visual-inertial sensor suite to first build a Reference Map of the UAV’s workspace during a piloted reconnaissance flight. In subsequent flights over this area, the proposed framework combines keyframe-based visual-inertial odometry with novel geometric image-based localization, to provide a real-time estimate of the UAV’s pose with respect to the Reference Map paving the way towards completely automating repeated navigation in this workspace. The stability of the system is ensured by decoupling the local visual-inertial odometry from the global registration to the Reference Map, while GPS feeds are used as a weak prior for suggesting loop closures. The proposed framework is shown to outperform GPS localization significantly and diminishes drift effects via global image-based alignment for consistently robust performance.Descriptors for image matching:  Zheng et al. [15] proposed an affine and rotation-invariant SIFT feature-based descriptor to perform matches between UAV and satellite images. This descriptor can vary the shape of the patch around a keypoint, becoming a robust descriptor with manageable computational complexity.  Calonder et al. [13] propose the use of binary strings as an efficient feature point descriptor, which the authors call BRIEF. We show that it is highly discriminative even when using relatively few bits and can be computed using simple intensity difference tests. Furthermore, the descriptor similarity can be evaluated using the Hamming distance, which is very efficient to compute, instead of the L2 norm as is usually done.Segmentation:  Valada et al. [17] use a Deep Convoluted Neural Network with multiple modalities to achieve state-of-the-art performance on datasets with adverse environmental conditions, proving robust and reliable segmentation with acceptable inference time for mobile robotics. A 4.52 km trail through the Freiburg forest was driven autonomously using only the segmented images from the AdapNet with their Convoluted Mixture of Deep Experts (CMoDE), demonstrating the reliability and robustness claimed above.Proposed method The localization framework consists of two main building blocks:A deep learning module is designed to extract certain semantic categories and produce synthetic images representing these categories.A particle filter is designed for performing localization on said synthetic images.The UAV captures an image, the deep learning module segments the image and feeds it into the MCL, which estimates the UAV position based on a segmented a priori known map. To lower the computational cost of the MCL, heading data from either the IMU or a possible Visual odometry system will also be feed into the MCL.Flowchart over the proposed framework. Note the segmentation network requires training pre-flight, also to lessen the computational work of the MCL, heading, and possibly height is input from either an IMU, Compass, or Visual odometry.The most important advantage that this method hopefully provides over former methods is robustness against weather and environmental changes, of which they do not provide.Semantic segmentationWhile MCL for UAVs with the use of satellite images has been proven to work with good results in previous works, the greatest shortcoming has been its lack of robustness against adverse environmental conditions such as differences in seasons, weather, and lighting. While classical digital image processing might improve robustness with regards to lighting conditions, deep learning has the potential to make the MCL robust against most dynamic environmental factors, as it can learn features invariant to environmental changes and output a semantic representation of both satellite images and UAV images.The first consideration for our implementation is U-net [10] as it is state-of-the-art on microscopic photography segmentation, who, like orthophotography, lacks three-dimensional features. Thus, we expect the U-Net to be performant on processed aerial images (orthophotos), as well, though other nets such as AdapNet [17], has shown to be better at segmenting multi-scale features, which could be important for a greater variance in drone height. There exist many variations of the U-net, with promising results that will also be taken into consideration.Data set:While many datasets for semantic segmentation of aerial and satellite images exist, very few contain the environment variances needed for our application. Thus another challenge is to modify or create a completely new dataset with the mentioned variances. As the Norwegian mapping organization, Kartverket provides detailed vectorized maps that can serve as ground truth for the datasets, see , little work remains to create a large and functional dataset. Producing maps of different seasons and weather can easily be done with a drone and image mosaicking. Combining data from several sets will also aid in high variance possibly reducing overfitting and increasing generalization.Example of Kartverkets vectorized images. Monte Carlo LocalizationThe proposed framework utilizes segmented images from a downward-facing camera and localizes them in a segmented orthophoto that is used as a global map. Our approach applies an MCL algorithm, to locate the images which will be described in this section. The pixel comparison model has been simulated and provided outstanding results given a perfect segmentation. The model creates a set P of k pixels placed on the same location at both the particle and robot images. The intensities of each pixel xi on both images are then compared and given a binary value depending on the pixels being equal or not. E.g if all pixels have the same intensity in both images, there is a high probability of the images being the same or similar depending on the number of pixels compared.Example of pixel comparison. Here, a set P containing k = 4 pixels. MCL OverviewGiven a map of the environment, the goal of the algorithm is for the robot to determine its pose within the environment.At every time t the algorithm takes as input the previous belief Xt − 1 = {xt − 1[1], xt − 1[2], …, xt − 1[M]}, an actuation command ut, and data received from sensors zt; and the algorithm outputs the new belief Xt. [18]MCL algorithm Optimizations and hardwareThe variables and data used in the MCL algorithm and the measurement model, are inherently vectorized. With this in mind, optimizing the code execution to utilize heterogeneous multicore architectures may have a tremendous positive impact on execution time.A development board such as nVIDIA’s Jetson Xavier is a very good candidate for this framework. It provides an ARM-based processor and 8 Volta streaming multiprocessors and consumes a total of 15 Watt at max power draw. The unit is proven to be very efficient in AI usage as it also has 64 Tensor Cores.Summary and future workThe proposed method, if it produces viable results, will provide global localizations in situations where former work has not been successful. The first step is to implement a raw version of the proposed method, see , and eventually optimize the solution on hardware if results are applicable. The most challenging and important part of the method is to provide a tolerable segmentation network, hence this is where most of the early work will reside.References[1] T. Murfin, “UAV Report: Growth Trends &amp;amp; Opportunities for 2019.” https://www.gpsworld.com/uav-report-growth-trends-opportunities-for-2019/, 2018, [Online]. Available: https://www.gpsworld.com/uav-report-growth-trends-opportunities-for-2019/.[2] “INS drift.” $$url{https://www.skybrary.aero/index.php/Inertial_Navigation_System_(INS)}.[3] J. V. Carroll, “Vulnerability assessment of the U.S. transportation infrastructure that relies on the global positioning system,” Journal of Navigation, vol. 56, no. 2, pp. 185–193, 2003, doi: 10.1017/S0373463303002273.[4] F. Caballero, L. Merino, J. Ferruz, and A. Ollero, “Improving vision-based planar motion estimation for unmanned aerial vehicles through online mosaicing,” in Proceedings - ieee international conference on robotics and automation, 2006, vol. 2006, pp. 2860–2865, doi: 10.1109/ROBOT.2006.1642135.[5] US Air Force, “GPS Accuracy.” https://www.gps.gov/systems/gps/performance/accuracy/, 2017, [Online]. Available: https://www.gps.gov/systems/gps/performance/accuracy/.[6] G. O’Dwyer, “Norway says it proved Russian GPS interference during NATO exercises.” https://www.defensenews.com/global/europe/2019/03/08/norway-alleges-signals-jamming-of-its-military-systems-by-russia/.[7] M. Mantelli et al., “A novel measurement model based on abBRIEF for global localization of a UAV over satellite images,” Robotics and Autonomous Systems, vol. 112, pp. 304–319, 2019, doi: 10.1016/j.robot.2018.12.006.[8] A. Masselli, R. Hanten, and A. Zell, “Localization of unmanned aerial vehicles using Terrain classification from aerial images,” in Advances in intelligent systems and computing, 2015, vol. 302, pp. 831–842, doi: 10.1007/978-3-319-08338-4_60.[9] A. Nassar, K. Amer, R. Elhakim, and M. Elhelw, “A deep CNN-based framework for enhanced aerial imagery registration with applications to UAV geolocalization,” in IEEE computer society conference on computer vision and pattern recognition workshops, 2018, vols. 2018-June, pp. 1594–1604, doi: 10.1109/CVPRW.2018.00201.[10] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for biomedical image segmentation,” in Lecture notes in computer science (including subseries lecture notes in artificial intelligence and lecture notes in bioinformatics), 2015, vol. 9351, pp. 234–241, doi: 10.1007/978-3-319-24574-4_28.[11] J. Ginés, F. Martín, V. Matellán, F. J. Lera, and J. Balsa, “3D Mapping for a Reliable Long-Term Navigation,” in Advances in intelligent systems and computing, 2018, vol. 694, pp. 283–294, doi: 10.1007/978-3-319-70836-2_24.[12] M. K. Hu, “Visual Pattern Recognition by Moment Invariants,” IRE Transactions on Information Theory, vol. 8, no. 2, pp. 179–187, 1962, doi: 10.1109/TIT.1962.1057692.[13] M. Calonder, V. Lepetit, C. Strecha, and P. Fua, “BRIEF: Binary robust independent elementary features,” in Lecture notes in computer science (including subseries lecture notes in artificial intelligence and lecture notes in bioinformatics), 2010, vol. 6314 LNCS, pp. 778–792, doi: 10.1007/978-3-642-15561-1_56.[14] A. Viswanathan, B. R. Pires, and D. Huber, “Vision-based robot localization across seasons and in remote locations,” in Proceedings - ieee international conference on robotics and automation, 2016, vols. 2016-June, pp. 4815–4821, doi: 10.1109/ICRA.2016.7487685.[15] M. Zheng, C. Wu, D. Chen, and Z. Meng, “Rotation and affine-invariant SIFT descriptor for matching UAV images with satellite images,” in 2014 ieee chinese guidance, navigation and control conference, cgncc 2014, 2015, pp. 2624–2628, doi: 10.1109/CGNCC.2014.7007582.[16] J. Surber, L. Teixeira, and M. Chli, “Robust visual-inertial localization with weak GPS priors for repetitive UAV flights,” in Proceedings - ieee international conference on robotics and automation, 2017, pp. 6300–6306, doi: 10.1109/ICRA.2017.7989745.[17] A. Valada, J. Vertens, A. Dhall, and W. Burgard, “AdapNet: Adaptive semantic segmentation in adverse environmental conditions,” in Proceedings - ieee international conference on robotics and automation, 2017, pp. 4644–4651, doi: 10.1109/ICRA.2017.7989540.[18] S. Thrun, “Probabilistic robotics,” Communications of the ACM, vol. 45, no. 3, pp. 52–57, 2002, doi: 10.1145/504729.504754.",
      "url": "/2020/09/22/Essay/",
      "author": "Vegard Bergsvik Øvstegård",
      "categories": ""
    }
    ,
  

    "2020-09-15-first-presentation": {
      "title": "First progress presentation",
      "content"	 : "First progress presentationVegard Bergsvik ØvstegårdFri - 18 Sep 2020 Days until delivery: 245 daysGIL-UAVGPS-Independent Localization for UAVsSystem diagramFramework simulationCurrent statusTaskProgressIn progress:1. Create U-net (PyTorch, multi-GPU)2. Acquire &amp;amp; improve DatasetTo do:3. Train the U-net4. Code dataset-producing software5. Get drone footage6. Implement framework(C++, SIMD, CUDA)CompletedImplement naive MCL algorithm (Python)Get hardware (nVIDIA Jetson TX1)Current resultsMCL simulation with current measurement model works like a charm.This is given a perfect segmentation, which will not occur IRL.Plan for the next fortnight: Week 39Week 40Finish U-net codeCode dataset-producing softwareAcquire &amp;amp; improve DatasetStart framework implementationTrain the U-netGet drone footageGet drone footagePrintBack",
      "url": "/2020/09/15/First-Presentation/",
      "author": "Vegard Bergsvik Øvstegård",
      "categories": ""
    }
    
  

  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
}
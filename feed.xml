<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>GIL-UAV</title>
    <description>GPS-Independent Localization for UAVs</description>
    <link>https://gil-uav.github.io//</link>
    <atom:link href="https://gil-uav.github.io//feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Mon, 14 Dec 2020 22:26:44 +0100</pubDate>
    <lastBuildDate>Mon, 14 Dec 2020 22:26:44 +0100</lastBuildDate>
    <generator>Jekyll v4.1.1</generator>
    
      <item>
        <title>Monte Carlo Localization Background</title>
        <description>Robot localization
Robot localization refers to a robot’s ability to establish its position and orientation within its frame of reference. Vision-based localization or optical localization uses computer vision techniques and algorithms, with optical sensors to extract features of the surrounding environment required to localize an agent.
Particle filter
A Particle filter is a type of algorithm used to solve states in dynamical systems[1]. They are often called Monte Carlo algorithms as they rely on repeated random sampling to obtain results. In other words, these algorithms use randomness to solve problems that might be deterministic in principle. Our problem is locating the UAV on a map, and a Monte Carlo approach is to compare the environment visible to the UAV with random locations on the map. The probability of the UAV being in an area is related to how similar the environment is around the UAV and a given random sample in the map.
Monte Carlo localization
Monte Carlo Localization, a.k.a particle filter localization[1], is an algorithm for robots to obtain pose and orientation. The algorithm uses a particle filter to represent probable states’ distribution, where each particle represents a possible state, i.e., a hypothesis of where the robot is[1]. As the robot has no information about where it is and assumes any pose and orientation is equally likely, the algorithm typically starts with a uniform random distribution of the configuration space. The particles shift whenever the robot moves to predict its next state. Every time the robot senses its environment, the particles are resampled based on how well the sensed data correlate with the predicted state. This resampling method is called recursive Bayesian estimation. Over time, the particles should converge towards the actual position of the robot[1].
Description
Consider a robot with an internal map of its surroundings. As the robot moves around, it needs to know its position within said map. Predicting its pose and orientation by using its sensor observations is known as previously mentioned, robot localization. Due to imperfections within the robot’s sensors, its mechanics, and perhaps the environment’s randomness, the movement might not behave entirely predictably. For this reason, it generates many random guesses on where heading. These guesses are known as particles. Each particle contains a full description of a future state, such as orientation, coordinates, etc. When the robot does an observation, it discards the guesses inconsistent with what the robot can see and generates more particles of those that appear consistent. Over time, the idea is that most particles, or guesses, converge to where the robot is.
Example of 1D robot:
Consider a robot in a 1D circular corridor with three identical doors, using a sensor that returns either true or false depending on whether there is a door.
t = 0

The algorithm initializes with a uniform distribution of particles. The robot considers itself equally likely to be at any point in space along the corridor, even though it is physically at the first door.


Sensor update: the robot detects a door. It assigns a weight to each of the particles. The particles which are likely to give this sensor reading receive a higher weight.


Resampling: the robot generates a set of new particles, with most of them generated around the previous particles with more weight. It now believes it is at one of the three doors.

t = 1

Motion update: the robot moves some distance to the right. All particles also move right, and some noise is applied. The robot is physically between the second and third doors.


Sensor update: the robot detects no door. It assigns a weight to each of the particles. The particles likely to give this sensor reading receive a higher weight.


Resampling: the robot generates a set of new particles, with most of them generated around the previous particles with more weight. It now believes it is at one of two locations.

t = 2

Motion update: the robot moves some distance to the left. All particles also move left, and some noise is applied. The robot is physically at the second door.


Sensor update: the robot detects a door. It assigns a weight to each of the particles. The particles likely to give this sensor reading receive a higher weight.


Resampling: the robot generates a set of new particles, with most of them generated around the previous particles with more weight. The robot has successfully localized itself.

At the end of the three iterations, most of the particles are converged on the actual position of the robot as desired.
Algorithm:
Given an a priori known map of the environment, the goal of the algorithm is for the robot to determine its pose within the environment.
At every time t the algorithm takes as input the previous belief Xt − 1 = {xt − 1[1], xt − 1[2], …, xt − 1[M]}, an actuation command ut, and data received from sensors zt; and the algorithm outputs the new belief Xt.[1]

Monte Carlo Localization algorithm [1]

GIL Simulation:
Below is a short simulation of the GIL-framework demonstrating MCL. Red dot is UAV and black dots are particles.




[1] S. Thrun, “Probabilistic robotics,” Communications of the ACM, vol. 45, no. 3, pp. 52–57, 2002, doi: 10.1145/504729.504754.


</description>
        <pubDate>Mon, 23 Nov 2020 00:00:00 +0100</pubDate>
        <link>https://gil-uav.github.io//mcl/2020/11/23/MCL-Background/</link>
        <guid isPermaLink="true">https://gil-uav.github.io//mcl/2020/11/23/MCL-Background/</guid>
        
        <category>Localization</category>
        
        
        <category>mcl</category>
        
      </item>
    
      <item>
        <title>Third progress presentation</title>
        <description>

Third progress presentation

Vegard Bergsvik Øvstegård
Fri - 13 Nov 2020
 
Days until delivery: 186 days





GIL-UAV
GPS-Independent Localization for UAVs

System diagram



Framework simulation






Status from previous progress presentation




Task
Progress




In progress:



1. Tune the U-net**



2. Acquire &amp;amp; improve Dataset*



3. Train the U-net*



To do:



4. Code dataset-producing software



5. Get drone footage



6. Implement framework(C++, SIMD, CUDA)



Completed



Create U-net (PyTorch, multi-GPU)*




Updated tasks* New tasks**


Updated tasks



Task
Progress




2. Acquire &amp;amp; improve Dataset



3. Train the U-net



4. Code dataset-producing software






Acquire &amp;amp; improve Dataset


Orthophotos:

Location: Norway
Year taken: 2018-2020
Ca 64 map-tiles.
Spanning over 200$ km^2$

Classes:

Buildings
Water bodies
Background




DS Version 0.0.1:

25,1Gb of data
67219 samples
Issues:

Poor building annotations due to rectification
Missing annotations on some buildings
Very inaccurate water annotations




DS Bump to Version 0.1.0:

Improvements:

Removed sets with images out of bounds from aerial photos.
Removed sets containing only background class.
Removed sets containing only water bodies class.

17,4Gb of data
43802 samples
Class balance:

Building percentage: 24 %
Water pixels: 6 %
Background pixels: 69 %




Code dataset-producing software

Features:

Automated extraction of ground truth labels from vector maps.
Generates datasets from orthophotos and GT images.
Discards (some)noisy sets.
Produces dataset statistics

Class balance
Mean
Standard deviation


Backlog:

Remove sets with non-annotated buildings.



Train the U-net

Test set example from newest dataset:

Images and predictions



Train the U-net


Ground truth and predictions



Current status and progress



Task
Progress




In progress:



1. Tune the U-net



2. Acquire &amp;amp; improve Dataset*



3. Train the U-net*



To do:



4. Get drone footage



5. Implement framework(C++, SIMD, CUDA)



Completed



Code dataset-producing software




Updated tasks* New tasks**


Completed tasks

Code dataset-producing software
Create U-net (PyTorch, multi-GPU)
Implement naive MCL algorithm (Python)
Get hardware (nVIDIA Jetson TX1)



Plan for the next fortnight:
 



Week 47
Week 48




Tune and train the U-net
Port MCL code from Python to C++


Get drone footage
Get drone footage


Acquire &amp;amp; improve Dataset
Create orthophotos from drone footage








Print
Back


</description>
        <pubDate>Thu, 12 Nov 2020 00:00:00 +0100</pubDate>
        <link>https://gil-uav.github.io//2020/11/12/Third-Presentation/</link>
        <guid isPermaLink="true">https://gil-uav.github.io//2020/11/12/Third-Presentation/</guid>
        
        <category>presentation</category>
        
        
      </item>
    
      <item>
        <title>The dataset v0.1.0</title>
        <description>The results from training a U-net on dataset v0.0.1 were better than feared. I thought there would be too much noise and bad iterations from the dataset, as mentioned in the previous post. Despite the discussed issues, the network handled the dataset surprisingly well:
Training metrics:

F1 score from validation


Loss from validation

Predictions:

Orthophotos with predicted buildings from the test set.


Ground truth with predictions from the test set.

As can be seen on set number two on the figures above, another issue with the dataset occurred. When extracting sets, some of the raster maps were extracted out of bounds from the aerial imagery base. This resulted in some black parts of the maps. Another observation is that several sets contain images with the background class only. An important part of the network is distinguishing between buildings and water and what is not. However, these sets might only prolong the training time, as many pixels belonging to the background class exists in the sets containing the target classes. This also translates to sets containing only water. I believe that there are enough sets with both target classes than for these sets to be included.
Bumping the dataset to v0.1.0
Regarding what i just mentioned, the new dataset has no sets with images that contain black pixels(0,0,0) as these are out of bounds sets and might induce noise. Sets containing only background class or only water class have also been removed. Lastly the masks have synthetically changed their interpolation to nearest neighbour, and converted to grayscale images. This reduces their size, and removes any ambiguous pixels between object borders that were not part of any classes. I.e buildings etc in masks have hard edges, and not smooth transitions. Such transitions may affect training negatively, and cause problems.
The results from said improvements were a reduction of the dataset size from 25,1Gb to 17,4Gb, and from 67219 to 43802 samples. There are still improvements to be made on the variance of the dataset and the annotations’ accuracy. Said improvements are the goal for the next version.
Statistics:

Mean: [114.84165988 115.00874479 104.93408015]
Standard deviation: [47.1493354 42.40584163 38.70938704]
Building pixels: 2787227245
Water pixels: 744719009
Background pixels: 7950485234
Building percentage: 24 %
Water pixels: 6 %
Background pixels: 69 %

</description>
        <pubDate>Wed, 11 Nov 2020 00:00:00 +0100</pubDate>
        <link>https://gil-uav.github.io//semantic_segmentation/2020/11/11/Dataset-V2/</link>
        <guid isPermaLink="true">https://gil-uav.github.io//semantic_segmentation/2020/11/11/Dataset-V2/</guid>
        
        <category>post</category>
        
        <category>machine-learning</category>
        
        
        <category>semantic_segmentation</category>
        
      </item>
    
      <item>
        <title>The dataset v0.0.1</title>
        <description>As stated previously in this work, the network’s goal is to segment static objects with a high amount of robustness. With this in mind, the dataset must be large and diverse enough throughout several factors such as location, lighting ,and seasonal variance. This post is a short update on the fist iteration of such a dataset.
Data acquisition
The dataset contains orthophotos covering different areas of Norway. All images came from the Norwegian Mapping Authority, Kartverket, via the University of Oslo. Pictures were taken in spatial resolution of 4 to 20 cm per pixel with three spectral bands RGB. They come from different years (2018 - 20220) and flights. The photo-flying season in Norway begins in April and lasts until the end of September. Therefore the acquired photos are characterized by a wide variety of optical conditions. They include images of different saturation, angles of sunlight, and shadows lengths. Simultaneously, the photos are from varying periods of the vegetation season. It does aid this dataset in providing robustness. However winter-landscapes and other challenging conditions are missing. To provide more diversity in the dataset, 64 map tiles were selected from various locations around all 11 different counties, spanning over 200km².
















No. sheets
Scale
GSD
Height
Width
Area height
Area length
Total size
Drone FOV
Drone height
No. sets




11
1:1000
4.0 cm
20000 px
15000 px
800 m
600 m
9.9 Gb
20 m
43 m
12441


11
1:2000
8.0 cm
20000 px
15000 px
1600 m
1200 m
9.9 Gb
41 m
87 m
12441


11
1:2000
10.0 cm
16000 px
12000 px
1600 m
1200 m
6.6 Gb
51 m
109 m
7843


11
1:5000
12.5 cm
25600 px
19200 px
3200 m
2400 m
11.0 Gb
64 m
136 m
20350


11
1:5000
20.0 cm
16000 px
12000 px
3200 m
2400 m
6.6 Gb
102 m
217 m
7843



NB! Drone field of view (FOV) and height are approximations.
Classes
As mentioned in earlier posts, the images are annotated with two classes: water bodies and buildings. Only roads longer than 50m were available in the vector maps, so they are not usable for this work.
Building
An object standing permanently in one place. RGB(255,255,255)
Water bodies
Flowing and stagnant water, including ponds and pools. RGB(127,127,127)
Background
Areas not classified into any class. It can include, e.g., fields, roads, and all objects excluded from above. RGB(0,0,0)
Annotations
The ground truth labels are annotated automatically using the vector maps provided by Kartverket. However, this does provide some issues as the annotations are set per GPS points, and the orthophotos are not always perfectly rectified. Also, water bodies are not always stationary and are prone to poor annotations.
1. Rectification of orthophotos was not perfect; some buildings were not completely segmented.  2. Not all buildings were segmented. 3. Water segmentation of rivers was highly inaccurate; this is due to the riverbed naturally shifting.
Summary
After cropping sets of images and annotations from the orthophotos and annotations maps, the dataset ended up with 67219 samples at a size of 25,1Gb. However, the dataset is not yet scraped for sets not containing useful classes, erroneous annotations, or missing orthophoto coverage. This is the first Alpha version, and the next iteration will most likely be smaller and hopefully more precise.
</description>
        <pubDate>Mon, 09 Nov 2020 00:00:00 +0100</pubDate>
        <link>https://gil-uav.github.io//semantic_segmentation/2020/11/09/Dataset-V1/</link>
        <guid isPermaLink="true">https://gil-uav.github.io//semantic_segmentation/2020/11/09/Dataset-V1/</guid>
        
        <category>post</category>
        
        <category>machine-learning</category>
        
        
        <category>semantic_segmentation</category>
        
      </item>
    
      <item>
        <title>Dataset generation pt. 3</title>
        <description>In order to extract the ground truth (GT) images from the vector maps recieved by Kartverket. I used QGIS, an open source geographic information system. It was how-ever quite tedious to to extract the GT images manually using the GUI. So i wrote some simple python functions to help me automate it. To put it short, the functions import all the orthophoto maps as layers to QGIS, and exports the images with the same spatial extent said orthophotos, but from the GT vector maps.
Map issues
I did discover some issues while inspecting the GT vector maps and the orthophotos:
1. Rectification of orthophotos was not perfect, some buildings were not completely segmented.  2. Not all buildings where segmented. 3. Water segmentation of rivers where highly inaccurate, this is due to the riverbed naturally shifting.
To solve issue 1, i used QGIS to adjust the GT vector layer a bit. I simply widened the vector by one meter at scale. The only way to solve issue number 2 and 3 is either with manual adjustment of the vector maps, or simply scrubbing the dataset after creation. Both tedious jobs, but they might be worth while. An idea to automate the scrubbing for issue number 2. is to train the network on the dataset, and then use the network identify and remove sets with non-segmented buildings. These fixes will be considered after the first training runs.
QGIS helper functions:

import os

from qgis.core import QgsProject
from glob import glob

project = QgsProject.instance()
root = QgsProject.instance().layerTreeRoot()


def import_map(path_to_tif: str, root):
    &amp;quot;&amp;quot;&amp;quot;
    Imports a .tif file as map-layer.

    Parameters
    ----------
    path_to_tif : str
        Path to .tif file.
    root
        Project instance layer root.

    Returns
    -------

    &amp;quot;&amp;quot;&amp;quot;
    rlayer = QgsRasterLayer(
        path_to_tif, os.path.basename(path_to_tif).replace(&amp;quot;.tif&amp;quot;, &amp;quot;&amp;quot;)
    )
    if not rlayer.isValid():
        print(&amp;quot;Layer failed to load!&amp;quot;)
    iface.addRasterLayer(path_to_tif, os.path.basename(path_to_tif).replace(&amp;quot;.tif&amp;quot;, &amp;quot;&amp;quot;))


def import_all_maps(path):
    &amp;quot;&amp;quot;&amp;quot;
    Imports all .tif files to project as layers.
    Parameters
    ----------
    path : str
        Path to folder containing .tif files(Ortophotos)

    Returns
    -------

    &amp;quot;&amp;quot;&amp;quot;
    maps = [y for x in os.walk(path) for y in glob(os.path.join(x[0], &amp;quot;*.tif&amp;quot;))]
    for m in ops:
        import_map(m, root)
        print(&amp;quot;{} imported.&amp;quot;.format(os.path.basename(m)))


def get_ortophoto_layers():
    &amp;quot;&amp;quot;&amp;quot;
    Returns all ortophoto layers from project.
    Returns
    -------
    layers : list
        List of ortophoto layers.

    &amp;quot;&amp;quot;&amp;quot;
    layers = [
        l
        for l in QgsProject.instance().mapLayers().values()
        if l.name().startswith(&amp;quot;33&amp;quot;)
    ]
    layers = [
        l
        for l in QgsProject.instance().mapLayers().values()
        if l.type() == QgsMapLayer.RasterLayer
    ]
    return layers


def export_basedata_as_img(layer, export_path: str):
    &amp;quot;&amp;quot;&amp;quot;
    Saves ground-truth for layer as .png file.

    Parameters
    ----------
    export_path : str
        Where to save the image.
    layer
        Orthophoto layer to produce ground-truth map from.

    &amp;quot;&amp;quot;&amp;quot;
    outfile = os.path.join(export_path, &amp;quot;{}_y.png&amp;quot;.format(layer.name()))

    settings = QgsMapSettings()
    settings.setLayers(
        [
            QgsProject.instance().mapLayersByName(&amp;quot;fkb_bygning_omrade&amp;quot;)[0],
            QgsProject.instance().mapLayersByName(&amp;quot;fkb_vann_omrade&amp;quot;)[0],
        ]
    )
    settings.setBackgroundColor(QColor(0, 0, 0))
    settings.setOutputSize(QSize(layer.width(), layer.height()))
    settings.setExtent(layer.extent())
    render = QgsMapRendererParallelJob(settings)

    def finished():
        img = render.renderedImage()
        img.save(outfile, &amp;quot;png&amp;quot;)

    render.finished.connect(finished)
    render.start()
    print(&amp;quot;Ground truth image export of {} started.&amp;quot;.format(layer.name()))
    from qgis.PyQt.QtCore import QEventLoop

    loop = QEventLoop()
    render.finished.connect(loop.quit)
    loop.exec_()
    print(&amp;quot;Ground truth image of {} exported to: {}&amp;quot;.format(layer.name(), outfile))


def export_all_ground_truth_maps(export_path: str):
    &amp;quot;&amp;quot;&amp;quot;
    Exports ground truth images of all orthophoto layers.

    export_path : str
        Where to save the image.
    &amp;quot;&amp;quot;&amp;quot;
    for l in get_ortophoto_layers():
        export_basedata_as_img(l, export_path)
</description>
        <pubDate>Thu, 05 Nov 2020 00:00:00 +0100</pubDate>
        <link>https://gil-uav.github.io//semantic_segmentation/2020/11/05/Dataset-Generation-Pt3/</link>
        <guid isPermaLink="true">https://gil-uav.github.io//semantic_segmentation/2020/11/05/Dataset-Generation-Pt3/</guid>
        
        <category>post</category>
        
        <category>machine-learning</category>
        
        
        <category>semantic_segmentation</category>
        
      </item>
    
      <item>
        <title>Dataset generation pt. 2</title>
        <description>As stated in the former post, the plan was to build the dataset based on an order of orthophotos with a Ground Sampling Distance(GSD) of 20cm. However, a decision was made to train the network on either 512x512 or 1024x1024 images. This to hopefully maximize efficiency and memory usage on the GPUs while training. A new order is in place, with the following specs:

Order specifications
















No. sheets
Scale
GSD
Height
Width
Area height
Area length
Total size
Drone FOV
Drone height
No. sets




11
1:1000
4.0 cm
20000 px
15000 px
800 m
600 m
9.9 Gb
20 m
43 m
12441


11
1:2000
8.0 cm
20000 px
15000 px
1600 m
1200 m
9.9 Gb
41 m
87 m
12441


11
1:2000
10.0 cm
16000 px
12000 px
1600 m
1200 m
6.6 Gb
51 m
109 m
7843


11
1:5000
12.5 cm
25600 px
19200 px
3200 m
2400 m
11.0 Gb
64 m
136 m
20350


11
1:5000
20.0 cm
16000 px
12000 px
3200 m
2400 m
6.6 Gb
102 m
217 m
7843



NB! Drone field of view (FOV) and height are approximations.
This order will provide the dataset with a total of 60918 different sets with a height variance from about 40m to 220m. The map sheets will be extracted from different locations across each county in Norway to provide some locational variance as well. The most important aspect when choosing locations is to get as many different roof-tops and buildings as possible. Ranging from industrial, residential, and other buildings such as cabins. The latter might be the hardest for the network to segment as many cabins in Norway have turfed roofing that looks very similar to the ground below.

Example of a city center


Example of turfed roofing


Example industrial area


Example of a residential area

These are the general chosen locations:

Agder: Kristiansand
Innlandet: Hamar
Møre og Romsdal: Ålesund
Nordland: Bodø
Oslo: Oslo
Rogarland: Stavanger
Vestfold og Telemark: Rjukan
Troms og Finnmark: Kirkenes
Trøndelag: Trondheim
Vestland: Bergen
Viken: Sarpsborg

</description>
        <pubDate>Fri, 30 Oct 2020 00:00:00 +0100</pubDate>
        <link>https://gil-uav.github.io//semantic_segmentation/2020/10/30/Dataset-Generation-Pt2/</link>
        <guid isPermaLink="true">https://gil-uav.github.io//semantic_segmentation/2020/10/30/Dataset-Generation-Pt2/</guid>
        
        <category>post</category>
        
        <category>machine-learning</category>
        
        
        <category>semantic_segmentation</category>
        
      </item>
    
      <item>
        <title>Dataset generation</title>
        <description>Background
In machine learning, a data set is a collection of data. In this work, it is a collection of sets of 2 images, x, and y. Where x, the input image, is an orthophoto while the other, y, is the ground truth of buildings and other classes.

Example of a set with x(left) and y(right)

While there are many open-source and publicly available datasets, that contain such sets of images. It is difficult finding datasets that have the specifications needed to train such a network that this work requires. E.g lack of environmental variance, poor resolution, and different labels/ground truth.
Therefore, a completely new dataset with the following requirements has to be created:

Ground truth containing at least buildings.

Water, roads, and other static objects are welcome.

Environmental variance such as:

Weather
Season
Time
Location

At least 1000 images per, class, weather state, season, time, and location.
Good resolution and a low ground sample distance.

Getting the data
As a student at the University of Oslo, one can be provided access to some of the data not available to the public at the Norwegian Mapping Authority, Kartverket. This not only provides vectorized maps working as very good ground truths for buildings and water but also some very decent orthophotos. However, they are all taken at a time when there is little to no occlusion from leaves, snow, etc, and the timing of day to provide the best possible lighting conditions for photography. I.e lacking the environmental variance needed. The aforementioned vectorized maps did also include segmented roads, but only public roads and not drive-ways to private properties as so on. Public roads and private driveways are usually both made of asphalt, and look the same. There is a suspicion that having, say only half the valid data classified as true will cause too much noise during training. None the less a very good starting point, and with tools like QGIS, it is possible to apply orthophotos collected elsewhere and maps created using photographic mosaic and a drone.
Ground sampling distance and field of view
As the Norwegian Civil Aviation Authority states, the max flight altitude in Norway for uncertified and private drones is a maximum of 120 meters relative to the ground. Given the specifications on a typical drone such as the DJI Mavic 2 Pro, this results in a horizontal viewing field of approximately 57m by 32m on a 16:9 aspect ratio, and a Ground Sampling Distance (GSD) 2.95cm:
$$ GSD_w = \frac{S_w \times H}{F_r \times im_W} = 2.95cm $$
$$
\begin{align*}
D_w = GSD \times im_W = 57m
\end{align*}
$$ $$
\begin{align*}
D_h = GSD \times im_H = 32m
\end{align*}
$$

Dw - Horizontal plane footprint width
Dh - Horizontal plane footprint height
Sw = 13.2 - Sensor width(mm)
H = 120 - Height(m)
Fr = 28 - Focal length(mm)
imW = 1920 - Image width(px)
imH = 1080 - Image height(px)

These specifications are somewhat important as they describe what the network will work on and most likely should train on. Ground sampling distance (GSD) is the “real-life” spatial distance between each pixel center. Some orthophotos from Kartverket can be downloaded with a GSD down to 4cm, and with a 4:3 aspect ration on full HD (1440px by 1080px), gives us a GSD of about 3.93. However, with this spatial resolution and an area of 3.2km by 2.4km, it would result in an uncompressed TIFF file of about 14Gb according to Kartverket. As the objective is to segment buildings, it seems that this high-resolution is not necessary. Then again, this work is research and such inquiries must be sought out. Below are two images comparing the GSD of 25cm and 44cm.

GSD = 25cm


GSD = 44cm

Previous work such as Sahu et al. [1] used GSD as high as 100cm with decent results, on a very similar task as this. On the other hand Horwath et al [2]. mentions improvements in accuracy for high resolution images in electron microscopy images, but not without challenges. The aforementioned task is although not directly transferable to segmenting buildings, as they were looking for very small particles with a radius of circa 4-6 pixels in a 1024x1024 image. E.g a small shed with a size of 1m2 would take up about 20x20 pixels, and most static buildings are bigger than 1m2.
Another point is that the network will run on a drone with limited computational resources. Floating-point operations are not free and unlimited. For this reason images with a GSD of 20 will suffice as a start, and if need be, access to higher resolution images is possible. As previously mentioned, images captured with a drone might also be added due to the lacking environmental variance that the images from Kartverket have. These will have almost the same GSD calculated above as the drone that will be used is a DJI Mavic Pro.
50 Gb of raster maps have been ordered, and the dataset will be adapted and updated continuously until adequate results on real drone photos have been produced by a potential network. With this in mind, the test-set will mostly contain drone footage as this is what the network will process when the framework is in production. The first version of the dataset is hopefully finished by the end of next week. The complete version, however, will not be ready until enough environmental variance is captured, and to no one’s surprise, time nor weather can be controlled.


[1] M. Sahu and A. Ohri, “VECTOR MAP GENERATION FROM AERIAL IMAGERY USING DEEP LEARNING,” ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences, vols. IV-2/W5, no. 2/W5, pp. 157–162, May 2019, doi: 10.5194/isprs-annals-IV-2-W5-157-2019.


[2] J. P. Horwath, D. N. Zakharov, R. Mégret, and E. A. Stach, “Understanding important features of deep learning models for segmentation of high-resolution transmission electron microscopy images,” npj Computational Materials, vol. 6, no. 1, Dec. 2020, doi: 10.1038/s41524-020-00363-x.


</description>
        <pubDate>Wed, 28 Oct 2020 00:00:00 +0100</pubDate>
        <link>https://gil-uav.github.io//semantic_segmentation/2020/10/28/Dataset-Generation/</link>
        <guid isPermaLink="true">https://gil-uav.github.io//semantic_segmentation/2020/10/28/Dataset-Generation/</guid>
        
        <category>post</category>
        
        <category>machine-learning</category>
        
        
        <category>semantic_segmentation</category>
        
      </item>
    
      <item>
        <title>Semantic Segmentation Background</title>
        <description>Semantic Segmentation

Example of Semantic images segmentation where buildings have been segmented(white).

The goal of semantic image segmentation is to classify each pixel of an input image with a corresponding class of what is being represented. Because we’re predicting for every pixel in the image, this task is commonly referred to as dense prediction. The expected output in semantic segmentation is a complete high resolution image in which all the pixels are classified.
Convolutional Neural Networks
A Convolutional Neural Network (CNN), in short, is a deep learning algorithm commonly used to, assign importance and differentiate between various objects and aspects of an image fed into it. It does so by changing and updating inherent weights and biases, based on a ground truth via supervised learning.
To some extent, they are very similar to regular neural networks that use Multilayer perceptrons(MLPs), both are made up of learnable weights. But contrary to MLPs, CNNs have an architecture that explicitly assumes its inputs have structures like images. This allows encoding said property into the architecture, by sharing the weights for each location in the image and having neurons respond only locally. I.e a CNN is composed of convolutional layers without any fully-connected layers or MLPs usually found at the end. This provides efficiency for the forward pass implementation, and most importantly reduces the number of parameters in the network compared to a fully connected network(FCN). E.g if a 3-channel image of size 256 by 256 pixels were to be feed into an FCN, it would require the first hidden layer to have 196608 input weights.

A common outline of a very shallow Convolutional Neural Network. One Convolutional layer and one Pooling layer. Input images are depicted as a 3D block as they often have 3 channels(RGB).

The pre-processing required in a CNN is also much lower as compared to other classification algorithms. While in primitive methods filters are hand-engineered, with enough training, CNNs can learn these filters. The architecture of a CNN is analogous to that of the connectivity pattern of Neurons in the Human Brain and was inspired by the organization of the Visual Cortex. Individual neurons respond to stimuli only in a restricted region of the visual field known as the Receptive Field. A collection of such fields overlap to cover the entire visual area. In particular, a node in a layer is only connected to a small region of the previous layer, contrary to each node in the layer before in a FCN. As mentioned, this reduces the number of parameters vastly.
Forward propagation
Forward propagation in a CNN architecture consists mostly of convolutional layers, pooling layers, and activation functions.
Convolution is the first layer to extract features from an input image or a previous layer. Convolution preserves the relationship between pixels by learning image features using small filters, also known as kernels, passing them over the image. I.e layer values from small areas in the previous layer or input images are used to calculate new values in the current layer. The small area is covered by a kernel of trainable weights, and the use of a compact area allows the new value in the current layer to retain surrounding information from that area in the former layer.
Pooling layers offer an approach to downsample areas, known as feature maps, by summarizing the presence of the different values in patches of the feature map. Two common pooling methods are average pooling and max pooling that summarize the average presence of a feature and the most activated presence of a feature respectively. In short, it is an operation whose objective is to reduce spatial size of the input by selecting a certain value and discarding the rest.
Activation functions are usually somewhat simple functions implemented at the end of a layer. They are highly important as they introduce non-linearity to the networks, which allow the layer and neurons to learn and pass answers down the pipeline.
Commonly, the last layers of CNNs are fully connected layers to make predictions. However, in this work, other modules are used in the aft parts of the architecture. For this reason, there are no further mentions of FCNs. The output layer is generally a Softmax layer to clamp the class scores to a value between 0 and 1. The aforementioned layers are described in this subsection.
Convolutional layer
Convolutional layers are the most important building blocks used in CNNs, hence their name[1]. In the context of a CNN, convolution is a linear operation that involves the multiplication of a set of weights with the input, much like a traditional neural network. Given that the technique was designed for two-dimensional input, the multiplication is performed between an array of input data and a two-dimensional array of weights, called a filter or a kernel.

The outline of a convolutional layer.

The filter is smaller than the input data, also the type of multiplication applied between a filter-sized patch of the input and the filter is a dot product. This multiplication produces a single scalar, resulting in a two-dimensional activation map. The spatial size of this activation map depends on whether the input was padded or not. Padding refers to the number of pixels added to an image when it is being processed by the kernel. Adding padding to an image processed by a CNN allows for a more accurate analysis of images, and allow the spatial size of the resulting activation map to be the same as the input.
Pooling Layer
Pooling is a common feature imbibed into CNN architectures, and the main idea behind these layers is to sum-up features from maps generated by previous convolution layers[1]. Formally its function is to reduce the spatial size of the activation maps, such that it reduces the number of parameters and hence computation in the network.
The most common form is max pooling. In a relatively simple operation, a kernel of the chosen size is applied as a sliding window across each activation map individually. The largest value within the area the kernel is applied to in the activation map is then chosen as the representative for that area. The kernel size and the stride are hyperparameters chosen by the designer of the architecture. The stride tells of how many pixels the kernel jump after each application. If the stride is equal to the number of rows in the kernel, none of the areas the kernel is applied to will overlap. In a max-pool layer, this is the most common approach.

Example of max-pooling in a single channel activation map with kernel sizes of 2x2, and stride of 2.

Max pooling is done in part to help over-fitting by providing an abstracted form of the activation maps. As well as it reduces the computational cost by reducing the number of parameters to learn, and it also provides basic translation invariance to the internal representations. Max pooling is done by applying a max filter over non-overlapping subregions of the initial activation maps.
The idea is to retain the information which best describes the context of the image, from each region and throw away the information which is not important.
Rectified Linear Unit
The Rectified Linear Unit(ReLu) is a non-linear activation function that is used in multi-layer neural networks or deep neural networks. For input values of x the function can be represented as:
$$
\begin{align*}
f(x) = max(0,x)
\end{align*}
$$(1) 
According to equation 1, the output of ReLu is the maximum value between zero and the input value. The output is equal to zero when the input value is negative and the input value when the input is positive. I.e
$$
\begin{align*}
f(x) = \begin{cases}
x, \mbox{if } x \geq 0\\
0, \mbox{otherwise}
\end{cases}
\end{align*}
$$(2) 
Traditionally, some prevalent non-linear activation functions, like sigmoid functions and hyperbolic tangent, have been used in neural networks to get activation values for each neuron. However, the ReLu function has become a more popular activation function because it can accelerate the training speed of deep neural networks compared to traditional activation functions. This because the derivative of ReLu is 1 for positive input. Owing to a constant, deep neural networks do not need to take additional time for computing error terms during the training phase.
The ReLu function does not trigger the vanishing gradient problem when the number of layers grows. This is because the function does not have an asymptotic upper and lower bound. Thus, the earliest layer (the first hidden layer) can receive the errors coming from the last layers to adjust all weights between layers. By contrast, a traditional activation function like sigmoid is restricted between 0 and 1, so the errors become small for the first hidden layer. The mentioned scenario will lead to a poorly trained neural network.
Normalization Layer
One of the most common Normalization techniques used nowadays is Batch Normalization (BN). It is a strategy that normalizes interlayer outputs in a neural network. This in effect resets the distribution of the output from previous layers to be more efficiently processed by the subsequent layers[2].
It relieves numerous problems with properly initializing neural networks. In practice, networks that use BN are significantly more robust to bad initialization. Additionally, BN can be interpreted as doing preprocessing at every layer of the network, but integrated into the network itself in a differentiable manner[3].
The method leads to faster learning rates, as normalization ensures that there are no extreme activation values. It also allows each layer to learn independently from others, and it reduces the amount of data lost between the processing layer. This improves learning accuracy throughout the network. Ioeffe and Szegezy [4] report a state of the art classification model that achieved the same accuracy but requiring 14 times fewer learning iterations to do so using BN.
However, according to Wu et al. [5] using batch sizes (BS) less than 32 with BN results in a dramatically increased model error. There are situations that one has to settle for sizes of BS less than 32. I.e when memory consumption of each data sample is too high, with large networks or simply with lacking hardware requirements. This work does handle somewhat high-resolution images, and for this reason, alternatives to BN which work well with small batch size are needed. Group Normalization (GN), proposed by Wu et al. is one of the latest normalization methods that avoids exploiting the batch dimension, thus is independent of batch size. They report that with a ResNet-50 model trained in the ImageNet training set using 8 GPUs, a reduction in BS from 32 to 2 resulted in little to no change in error with GN contrary to an increase in error when using BN.

ImageNet classification error vs. batch sizes. [5] 

Softmax layer
In many neural networks, the last layer is often a softmax layer[2]. It is used to transform values of the class scores to numbers ranging from 0 to 1. The sum of the class-wise predictions is 1, so the layer can be interpreted as a probability spread function and looks like this: $$f_j(\mathbf{z}) = \frac{e^{z_j}}{\sum_ke^{z_k}}$$(3)  where z is the set of scores to be squashed. As one can see, the formula takes each class score in the power of e, and divides it by the sum of the entire set in the power of e.
Backpropagation
Backpropagation, short for backward propagation of errors is arguably the most important part of the training process. This is where the learnable parameters, weights, and biases in the network are updated to make improved predictions [6]. When a network is run through the training loop, a loss function is calculated, which represents the network’s predictions and its distance from the true labels. Backpropagation allows us to calculate the gradient of the loss function, proceeding backward throughout the network from the last layer to the first. For the gradient to be calculated at a particular layer, the gradients of all following layers are combined via the chain rule of calculus. This enables each weight to be updated individually to, gradually reduce the loss function over many training iterations. Loss functions are described in more detail further down in this section.
The loss function provides the loss, or error, L at each output node, and the objective is to minimize this value. The input to this function is x and consists of the input values in the training data and the learnable parameters in the network. The input data is static and cannot be altered to minimize L, but the parameters are. The backpropagation method calculates how much the parameters should be altered, by finding their gradients relative to the error output from the loss function. Said gradient is calculated by taking the partial derivatives of the output with respect to input values. E.g if an output depends on three input values, the output has three partial derivatives. The gradient itself is the vector consisting of all these partial derivative values.
To attain the gradient, partial derivatives between each parameter relative to the error output that the particular parameter contributed to must be calculated. For this to be possible, the process must start at the output layer. Every node obtains input values from a set of nodes in the previous layer. In the forward pass, each node calculates its output values based on the input and the gradient of the input values relative to its output value. During backpropagation, moving backward from the output layer to the input layer. All the nodes in time learn the gradient of its output value relative relative to output of the value it contributed to calculating. Per the chain rule, the aforementioned gradient should then be multiplied with all the local gradients that the node has obtained. The process hence repeats when the input nodes to the particular node know the gradient of its output relative to the final error output value. In this manner, the gradient for every parameter from the output to the input layer will know its gradient relative to the final error output value of which they contributed to, and can be modified to minimize said error based on the gradient value.
Loss functions
The loss function is used to determine the error or in other words the loss, between a network prediction and a given target value. It expresses how far off the network is at doing a correct prediction. It is usually expressed as a scalar that increases by how far off the model is. As the goal of the model is to perform correct predictions, the main objective of the training process is to minimize this error.
A common loss function is the cross-entropy loss function. Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. The function can be described as:
$$ -\sum^M_{c=1}y_{o,c}\log(p_{o,c}) $$(4) 
where M is the number of classes, y is the binary indicator if class label c is the correct prediction for the observation o and p is the predicted probability observation o With binary classification, where the number of classes M equals 2, the cross-entropy can be calculated as:
$$
\begin{align*}
-(y\log(p)+1-y)\log(1-p))
\end{align*}
$$(5) 
If M &amp;gt; 2 we calculate a separate loss for each class label per observation and sum the result.
In practice, this gives a high loss value to wrong predictions and a 0 loss value to the right predictions. This is the behavior that is wanted in a loss function since when minimized it will give better predictions. This is the base of the loss function used in this work.
Fully Convolutional Network
The goal of semantic image segmentation is to classify each pixel of an input image with a corresponding class of what is being represented. Because we’re predicting for every pixel in the image, this task is commonly referred to as dense prediction. The expected output in semantic segmentation is a complete high-resolution image in which all the pixels are classified.
In a typical convolutional network, the height and width of the input gradually reduce i.e downsampling, because of pooling. This helps the filters in the deeper layers to focus on a larger receptive field. However the depth, number of filters used, gradually increase which aids in extracting more complex features from the image. From the pooling layers, one can somewhat conclude that by downsampling, the model better understands the context presented in the image, but it loses the information of locality, i.e where said context is located. Thus if one were to use a regular convolutional network with pooling layers and dense layers, the locality information will be lost and one only retains the contextual information.
A Fully Convolutional Network(FCN) is a CNNs based network that progresses from coarse to fine inference to predict every pixel like semantic segmentation requires[7].
Deconvolution
To get an output that is expected in semantic segmentation, there is a need to convert, or upsample, the low-resolution information provided by a typical CNN to high resolution, and recover the locality information.
Deconvolution, sometimes also called Transposed Convolution or fractionally strided convolution, is such a technique to perform upsampling of an image with learnable parameters[8]. On a high level, transposed convolution is exactly the opposite process of a normal convolution i.e., the input volume is a low resolution image and the output volume is a high-resolution image. A normal convolution can be described as a matrix multiplication of input image and filter to produce the output image. In short, by taking the transpose of the filter matrix, it is possible to reverse the convolution process, hence the name transposed convolution.


[1] “CS231n Convolutional Neural Networks for Visual Recognition.” Accessed: Oct. 24, 2020. [Online]. Available: https://cs231n.github.io/convolutional-networks/.


[2] “Batch Normalization Definition | DeepAI.” Accessed: Oct. 23, 2020. [Online]. Available: https://deepai.org/machine-learning-glossary-and-terms/batch-normalization.


[3] “CS231n Convolutional Neural Networks for Visual Recognition.” Accessed: Oct. 23, 2020. [Online]. Available: https://cs231n.github.io/.


[4] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network training by reducing internal covariate shift,” in 32nd international conference on machine learning, icml 2015, Feb. 2015, vol. 1, pp. 448–456, [Online]. Available: https://arxiv.org/abs/1502.03167v3.


[5] Y. Wu and K. He, “Group Normalization,” International Journal of Computer Vision, vol. 128, no. 3, pp. 742–755, Mar. 2020, doi: 10.1007/s11263-019-01198-w.


[6] “Backpropagation Definition | DeepAI.” Accessed: Oct. 25, 2020. [Online]. Available: https://deepai.org/machine-learning-glossary-and-terms/backpropagation.


[7] J. Long, E. Shelhamer, and T. Darrell, “Fully Convolutional Networks for Semantic Segmentation,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 39, no. 4, pp. 640–651, Nov. 2014, [Online]. Available: http://arxiv.org/abs/1411.4038.


[8] “Fractionally-Strided-Convolution Definition | DeepAI.” Accessed: Oct. 25, 2020. [Online]. Available: https://deepai.org/machine-learning-glossary-and-terms/fractionally-strided-convolution.


</description>
        <pubDate>Sun, 25 Oct 2020 00:00:00 +0200</pubDate>
        <link>https://gil-uav.github.io//semantic_segmentation/2020/10/25/Semantic-Segmentation-Background/</link>
        <guid isPermaLink="true">https://gil-uav.github.io//semantic_segmentation/2020/10/25/Semantic-Segmentation-Background/</guid>
        
        <category>post</category>
        
        <category>machine-learning</category>
        
        
        <category>semantic_segmentation</category>
        
      </item>
    
      <item>
        <title>Second progress presentation</title>
        <description>

Second progress presentation

Vegard Bergsvik Øvstegård
Fri - 16 Oct 2020
 
Days until delivery: 214 days





GIL-UAV
GPS-Independent Localization for UAVs

System diagram



Framework simulation






Status from previous progress presentation




Task
Progress




In progress:



1. Create U-net (PyTorch, multi-GPU)



2. Acquire &amp;amp; improve Dataset



To do:



3. Train the U-net



4. Code dataset-producing software



5. Get drone footage



6. Implement framework(C++, SIMD, CUDA)



Completed



Implement naive MCL algorithm (Python)



Get hardware (nVIDIA Jetson TX1)






Updated tasks



Task
Progress




1. Create U-net (PyTorch, multi-GPU)



2. Acquire &amp;amp; improve Dataset



3. Train the U-net






Create U-net (PyTorch, multi-GPU) 

Base model and framework implemented with PyTorch and PyTorch Ignite
Features:

Gradient clipping
Gradient accumulation
Early stopping
Metrics
Distributed data parallel(DDP)
Multi GPU training
Training notification(Discord)
Learning rate scheduler (ReduceLROnPlateau)
Rescaling of images.




Create U-net (PyTorch, multi-GPU) 

Features:

Data augmentation for training(All are random to some degree):

Contrast
Brightness
Saturation
Noise; Gaussian, Salt &amp;amp; Pepper, Poission, Speckle.
Rotations($$n*\frac{pi}{2}$$)
Vertical and Horisontal flips.





Acquire &amp;amp; improve Dataset 

Extracted larger aerial photographs from existing data, of which will aid in expanding the dataset by several images.
Fine-tuned the ground truth segmentation images as they where not overlapping buildings enough.
Have finally gotten access to Kartverkets database: 



Acquire &amp;amp; improve Dataset 
Resultsnotes:

Ground truth is far from perfect even after fine-tuning. This does yield noise in the dataset, witch may lead to the following cases:

NEGATIVE: Learning gets corrupted and a suitable global optimum may never occur.
POSITIVE: Noise may induce better generalization for the network.

Real life scenarios contain occlusions in the images, and network may learn to draw buildings as squares despite say occluding trees.





Train the U-net 

Attained several good preliminary results despite the network not being tuned and with a lacking dataset.
Somewhat sceptical to the results. Test and validation data is completely separate, but they are very similar. I assume images from example drone-footage will not be as good.
Results look like overfitting, but no indications of it from metrics.



Train the U-net 
 Segmented buildings masked in white to the right.


Train the U-net 
 Ground truth left and prediction right.


Train the U-net 

Training rounds left, validation rounds right.  No indications of overfitting. Light blue had learning-rate of 0.001. Metric is Binary Cross Entropy with Logits(BCEWithLogitsLoss).


Train the U-net 
Resultsnotes:

Results appear to be overfitting, however without indications in metrics.
Image resolution plays a big part, 512x512px images gave far better results then 256x256px.

I am considering increasing image size further, how ever this will affect batch-size and memory usage on GPUs.

To much variation and randomness in data augmentation is disruptive for training. I.e to much noise results in no training(Duh).
Considering adding elastic transformations to data-augmentation as not all buildings are square.
Also considering adding grayscale transformations as the network appears to find red-bricked roof-tops much faster than any other. This might aid in a better global optimum.



Current status and progress




Task
Progress




In progress:



1. Tune the U-net**



2. Acquire &amp;amp; improve Dataset*



3. Train the U-net*



To do:



4. Code dataset-producing software



5. Get drone footage



6. Implement framework(C++, SIMD, CUDA)



Completed



Create U-net (PyTorch, multi-GPU)*




Updated tasks* New tasks**


Completed tasks

Create U-net (PyTorch, multi-GPU)
Implement naive MCL algorithm (Python)
Get hardware (nVIDIA Jetson TX1)



Plan for the next fortnight:
 







Week 43
Week 44




Tune and train the U-net
Tune and train the U-net


Acquire &amp;amp; improve Dataset
Get drone footage


Update master thesis with current results and findings
Create orthophoto maps from drone-footage








Print
Back


</description>
        <pubDate>Fri, 16 Oct 2020 00:00:00 +0200</pubDate>
        <link>https://gil-uav.github.io//2020/10/16/Second-Presentation/</link>
        <guid isPermaLink="true">https://gil-uav.github.io//2020/10/16/Second-Presentation/</guid>
        
        <category>presentation</category>
        
        
      </item>
    
      <item>
        <title>Pytorch(v&gt;=1.6.0) performance tuning tips</title>
        <description>Enable asynchronous data loading &amp;amp; augmentation
PyTorch DataLoader supports asynchronous data loading/augmentation. The defaults settings are with 0 threads and no pinned memory. Use num_workers &amp;gt; 0 to enable asynchronous data processing, and it’s almost always better to use pinned memory.
Default settings:
DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,
           batch_sampler=None, num_workers=0, collate_fn=None,
           pin_memory=False, drop_last=False, timeout=0,
       worker_init_fn=None)
This is faster.
DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,
           batch_sampler=None, num_workers=8, collate_fn=None,
           pin_memory=True, drop_last=False, timeout=0,
       worker_init_fn=None)
You know how sometimes your GPU memory shows that it’s full but you’re pretty sure that your model isn’t using that much? That overhead is called pinned memory. ie: this memory has been reserved as a type of “working allocation.” When you enable pinned_memory in a DataLoader it automatically puts the fetched data Tensors in pinned memory, and enables faster data transfer to CUDA-enabled GPUs.

Pinned memory

Enable cuDNN autotuner
For convolutional neural networks, enable cuDNN autotuner by setting:
torch.backends.cudnn.benchmark = True
cuDNN supports many algorithms to compute convolution, and the autotuner runs a short benchmark and selects the algorithm with the best performance.
Increase batch size
Often AMP reduces memory requirements due to half float precision utilization. Thus increase the batch size to max out GPU memory.
When increasing batch size: * Tune the learning rate * Add learning rate warmup and learning rate decay * Tune weight decay or switch to optimizer designed for large-batch training * LARS * LAMB * NVLAMB * NovoGrad
Disable bias for convolutions directly followed by a batch norm.

Disable bias

Use parameter.grad = None instead of model.zero_grad()
Not this:
model.zero_grad()
But this!
for param in model.parameters():
    param.grad = None
The former executes memset for every parameter in the model, and backward pass updates gradients with “+=” operator (read + write). This is a slow and naive implementation of PyTorch, will hopefully be fixed. The latter doesn’t execute memset for every parameter, memory is zeroed-out by the allocator in a more efficient way and backward pass updates gradients with “=” operator (write).
Disable debug APIS for final training
There are many debug APIs that might be enabled, this slows everything down. Here are some: * torch.autograd.detect_anomaly * torch.autograd.set_detect_anomaly(True) * torch.autograd.profiler.profile * torch.autograd.profiler.emit_nvtx * torch.autograd.gradcheck * torch.autograd.gradgradcheck
Use efficient multi-GPU backend
DataParalell uses 1 CPU core and 1 python process to drives multiple GPUs. Works for a single node, but even for that DistributedDataparallel is often faster. IT provides 1 CPU core for each GPU, likewise 1 python process for each GPU. It can do Single-node and multi-node (same API), has efficient implementation with automatic bucketing for grad all-reduce, all-reduce overlapped with backward pass and is for all intended purposes multi-process programming.
Fuse pointwise operations
PyTorch JIT can fuse pointwise operations into a single CUDA kernel. Unfused pointwise operations are memory-bound, for each unfused op PyTorch has to: * launch a separate CUDA kernel * load data from global memory * perform computation * store results back into global memory
I.e from this:
def gelu(x):
    return x * 0.5 * (1.0 + torch.erf(x / 1.41421))
to this:
@torch.jit.script
def fused_gelu(x):
    return x * 0.5 * (1.0 + torch.erf(x / 1.41421))
Construct tensors directly on GPUs
Dont do this:
t = tensor.rand(2,2).cuda()
Instead, create the tensor directly on the device:
t = tensor.rand(2,2, device=torch.device(&amp;#39;cuda:0&amp;#39;))
Its faster!
Summary:

use async data loading / augmentation
enable cuDNN autotuner
increase the batch size, disable debug APIs and remove unnecessary computation
efficiently zero-out gradients
use DistributedDataParallel instead of DataParallel
apply PyTorch JIT to fuse pointwise operations

</description>
        <pubDate>Wed, 23 Sep 2020 00:00:00 +0200</pubDate>
        <link>https://gil-uav.github.io//semantic_segmentation/2020/09/23/Pytorch-Optimizations/</link>
        <guid isPermaLink="true">https://gil-uav.github.io//semantic_segmentation/2020/09/23/Pytorch-Optimizations/</guid>
        
        <category>post</category>
        
        <category>optimizations</category>
        
        <category>machine-learning</category>
        
        
        <category>semantic_segmentation</category>
        
      </item>
    
  </channel>
</rss>

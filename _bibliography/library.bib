Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{sahu,
abstract = {Abstract. We propose a simple yet efficient technique to leverage semantic segmentation model to extract and separate individual buildings in densely compacted areas using medium resolution satellite/UAV orthoimages. We adopted standard UNET architecture, additionally added batch normalization layer after every convolution, to label every pixel in the image. The result obtained is fed into proposed post-processing pipeline for separating connected binary blobs of buildings and converting it into GIS layer for further analysis as well as for generating 3D buildings. The proposed algorithm extracts building footprints from aerial images, transform semantic to instance map and convert it into GIS layers to generate 3D buildings. We integrated this method in Indshine's cloud platform to speed up the process of digitization, generate automatic 3D models, and perform the geospatial analysis. Our network achieved {\&}sim;70{\%} Dice coefficient for the segmentation process.},
author = {Sahu, M. and Ohri, A.},
doi = {10.5194/isprs-annals-IV-2-W5-157-2019},
file = {:home/vegovs/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sahu, Ohri - 2019 - VECTOR MAP GENERATION FROM AERIAL IMAGERY USING DEEP LEARNING.pdf:pdf},
issn = {2194-9050},
journal = {ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences},
keywords = {Aerial images,Building footprint,Deep Learning,GIS,Segmentation,Vectorization},
month = {may},
number = {2/W5},
pages = {157--162},
publisher = {Copernicus GmbH},
title = {{VECTOR MAP GENERATION FROM AERIAL IMAGERY USING DEEP LEARNING}},
url = {https://www.isprs-ann-photogramm-remote-sens-spatial-inf-sci.net/IV-2-W5/157/2019/},
volume = {IV-2/W5},
year = {2019}
}
@misc{deepai_sm,
title = {{Softmax Function Definition | DeepAI}},
url = {https://deepai.org/machine-learning-glossary-and-terms/softmax-layer},
urldate = {2020-10-23}
}
@inproceedings{nassar,
abstract = {In this paper we present a novel framework for geolocalizing Unmanned Aerial Vehicles (UAVs) using only their onboard camera. The framework exploits the abundance of satellite imagery, along with established computer vision and deep learning methods, to locate the UAV in a satellite imagery map. It utilizes the contextual information extracted from the scene to attain increased geolocalization accuracy and enable navigation without the use of a Global Positioning System (GPS), which is advantageous in GPS-denied environments and provides additional enhancement to existing GPS-based systems. The framework inputs two images at a time, one captured using a UAV-mounted downlooking camera, and the other synthetically generated from the satellite map based on the UAV location within the map. Local features are extracted and used to register both images, a process that is performed recurrently to relate UAV motion to its actual map position, hence performing preliminary localization. A semantic shape matching algorithm is subsequently applied to extract and match meaningful shape information from both images, and use this information to improve localization accuracy. The framework is evaluated on two different datasets representing different geographical regions. Obtained results demonstrate the viability of proposed method and that the utilization of visual information can offer a promising approach for unconstrained UAV navigation and enable the aerial platform to be self-aware of its surroundings thus opening up new application domains or enhancing existing ones.},
author = {Nassar, Ahmed and Amer, Karim and Elhakim, Reda and Elhelw, Mohamed},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2018.00201},
isbn = {9781538661000},
issn = {21607516},
number = {June},
pages = {1594--1604},
title = {{A deep CNN-based framework for enhanced aerial imagery registration with applications to UAV geolocalization}},
volume = {2018-June},
year = {2018}
}
@misc{fsc,
title = {{Fractionally-Strided-Convolution Definition | DeepAI}},
url = {https://deepai.org/machine-learning-glossary-and-terms/fractionally-strided-convolution},
urldate = {2020-10-25}
}
@article{Milioto2019,
abstract = {The ability to interpret a scene is an important capability for a robot that is supposed to interact with its environment. The knowledge of what is in front of the robot is, for example, relevant for navigation, manipulation, or planning. Semantic segmentation labels each pixel of an image with a class label and thus provides a detailed semantic annotation of the surroundings to the robot. Convolutional neural networks (CNNs) are popular methods for addressing this type of problem. The available software for training and the integration of CNNs for real robots, however, is quite fragmented and often difficult to use for non-experts, despite the availability of several high-quality open-source frameworks for neural network implementation and training. In this paper, we propose a tool called Bonnet, which addresses this fragmentation problem by building a higher abstraction that is specific for the semantic segmentation task. It provides a modular approach to simplify the training of a semantic segmentation CNN independently of the used dataset and the intended task. Furthermore, we also address the deployment on a real robotic platform. Thus, we do not propose a new CNN approach in this paper. Instead, we provide a stable and easy-to-use tool to make this technology more approachable in the context of autonomous systems. In this sense, we aim at closing a gap between computer vision research and its use in robotics research. We provide an open-source codebase for training and deployment. The training interface is implemented in Python using TensorFlow and the deployment interface provides C++ library that can be easily integrated in an existing robotics codebase, a ROS node, and two standalone applications for label prediction in images and videos.},
archivePrefix = {arXiv},
arxivId = {1802.08960},
author = {Milioto, Andres and Stachniss, Cyrill},
doi = {10.1109/ICRA.2019.8793510},
eprint = {1802.08960},
isbn = {9781538660263},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
pages = {7094--7100},
title = {{Bonnet: An open-source training and deployment framework for semantic segmentation in robotics using CNNs}},
volume = {2019-May},
year = {2019}
}
@article{Exam2013,
author = {Exam, Final and White, Use},
isbn = {9781509046331},
pages = {2--3},
title = {{1 T " 1 !}},
year = {2013}
}
@article{Kummerle2008,
abstract = {We propose a novel combination of techniques for robustly estimating the position of a mobile robot in outdoor environments using range data. Our approach applies a particle filter to estimate the full six-dimensional state of the robot and utilizes multilevel surface maps, which, in contrast to standard elevation maps, allow the robot to represent vertical structures and multiple levels in the environment. We describe probabilistic motion and sensor models to calculate the proposal distribution and to evaluate the likelihood of observations. We furthermore describe an active localization approach that actively selects the sensor orientation of the two-dimensional laser range scanner to improve the localization results. To efficiently calculate the appropriate orientation, we apply a clustering operation on the particles and evaluate potential orientations on the basis of these clusters. Experimental results obtained with a mobile robot in large-scale outdoor environments indicate that our approach yields robust and accurate position estimates. The experiments also demonstrate that multilevel surface maps lead to a significantly better localization performance than standard elevation maps. They additionally show that further accuracy is obtained from the active sensing approach. {\textcopyright} 2008 Wiley Periodicals, Inc.},
author = {K{\"{u}}mmerle, Rainer and Triebel, Rudolph and Pfaff, Patrick and Burgard, Wolfram},
doi = {10.1002/rob.20245},
issn = {15564959},
journal = {Journal of Field Robotics},
number = {6-7},
pages = {346--359},
title = {{Monte Carlo localization in outdoor terrains using multilevel surface maps}},
volume = {25},
year = {2008}
}
@article{Naseer2015,
abstract = {In this paper, we present an appearance-based visual SLAM approach that focuses on detecting loop closures across seasons. Given two image sequences, our method first extracts one descriptor per image for both sequences using a deep convolutional neural network. Then, we compute a similarity matrix by comparing each image of a query sequence with a database. Finally, based on the similarity matrix, we formulate a flow network problem and compute matching hypotheses between sequences. In this way, our approach can handle partially matching routes, loops in the trajectory and different speeds of the robot. With a matching hypothesis as loop closure information and the odometry information of the robot, we formulate a graph based SLAM problem and compute a joint maximum likelihood trajectory.},
author = {Naseer, Tayyab and Ruhnke, Michael and Stachniss, Cyrill and Spinello, Luciano and Burgard, Wolfram},
doi = {10.1109/IROS.2015.7353721},
isbn = {9781479999941},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
keywords = {Databases,Feature extraction,Robustness,Simultaneous localization and mapping,Trajectory,Visualization},
pages = {2529--2535},
title = {{Robust visual SLAM across seasons}},
volume = {2015-Decem},
year = {2015}
}
@inproceedings{masselli,
abstract = {In this paper we investigate the benefit of terrain classification for selflocalization of a flying robot. The key idea is to use aerial images, which are already available from online databases such as GoogleMaps™, as reference map and to match images taken with a downward looking camera with this map. Using different terrain classes as features, we can make sure that our method is invariant to lighting/weather changes as well as seasonal variations or minor changes in the environment. A particle filter is used to register the query image with parts of the map. The proposed method has shown to work on image data from both simulated and real flights.},
author = {Masselli, Andreas and Hanten, Richard and Zell, Andreas},
booktitle = {Advances in Intelligent Systems and Computing},
doi = {10.1007/978-3-319-08338-4_60},
file = {:home/vegovs/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Masselli, Hanten, Zell - 2015 - Localization of unmanned aerial vehicles using Terrain classification from aerial images.pdf:pdf},
isbn = {9783319083377},
issn = {21945357},
keywords = {Terrain classification,Unmanned aerial vehicles,Visual localization},
pages = {831--842},
publisher = {Springer Verlag},
title = {{Localization of unmanned aerial vehicles using Terrain classification from aerial images}},
url = {https://link.springer.com/chapter/10.1007/978-3-319-08338-4{\_}60},
volume = {302},
year = {2015}
}
@misc{gps-jamming,
annote = {Accessed: 2020-09-10},
author = {O'Dwyer, Gerard},
howpublished = {https://www.defensenews.com/global/europe/2019/03/08/norway-alleges-signals-jamming-of-its-military-systems-by-russia/},
title = {{Norway says it proved Russian GPS interference during NATO exercises}}
}
@inproceedings{caballero,
abstract = {The paper presents a vision-based position estimation method for UAVs. It assumes a planar scene, approximation that usually holds when a vehicle is flying at a relatively high altitude. Monocular image sequences gathered by the UAV are used to estimate the vehicle motion, but accumulative errors can make diverge the estimated position. The proposed method uses an online-built mosaic to correct the drift associated to the planar motion estimation algorithm. The mosaic allows to use not only the current image but also previously recorded information for localization. Results from actual field experiments are presented. {\textcopyright}2006 IEEE.},
author = {Caballero, Fernando and Merino, Luis and Ferruz, Joaqu{\'{i}}n and Ollero, An{\'{i}}bal},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ROBOT.2006.1642135},
isbn = {0780395069},
issn = {10504729},
pages = {2860--2865},
title = {{Improving vision-based planar motion estimation for unmanned aerial vehicles through online mosaicing}},
volume = {2006},
year = {2006}
}
@inproceedings{ioffe,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82{\%} top-5 test error, exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
booktitle = {32nd International Conference on Machine Learning, ICML 2015},
eprint = {1502.03167},
file = {:home/vegovs/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ioffe, Szegedy - 2015 - Batch normalization Accelerating deep network training by reducing internal covariate shift.pdf:pdf},
isbn = {9781510810587},
keywords = {()},
month = {feb},
pages = {448--456},
publisher = {International Machine Learning Society (IMLS)},
title = {{Batch normalization: Accelerating deep network training by reducing internal covariate shift}},
url = {https://arxiv.org/abs/1502.03167v3},
volume = {1},
year = {2015}
}
@inproceedings{zheng,
abstract = {Image matching is a key issue in Vision-Based UAV navigation problems. This paper presents an affine and rotation-invariant SIFT features descriptor for matching UAV image with satellite images. The SIFT and ASIFT algorithm are nowadays widely applied for robust image matching, but it also has a high computational complexity. SURF is used for real-time UAV position estimation but is not satisfied for affine invariant. We introduce the new SIFT feature descriptor based on pie chart region. This descriptor is invariant for rotation, affine, scale and the dimension of the feature vector is relatively reduced. Therefore, this method satisfies robustness and low computational complexity. Experiments show that this method can improve the matching accuracy and robustness.},
author = {Zheng, Mingguo and Wu, Chengdong and Chen, Dongyue and Meng, Zhexiu},
booktitle = {2014 IEEE Chinese Guidance, Navigation and Control Conference, CGNCC 2014},
doi = {10.1109/CGNCC.2014.7007582},
isbn = {9781479946990},
pages = {2624--2628},
title = {{Rotation and affine-invariant SIFT descriptor for matching UAV images with satellite images}},
year = {2015}
}
@misc{stanford_cnn,
title = {{CS231n Convolutional Neural Networks for Visual Recognition}},
url = {https://cs231n.github.io/convolutional-networks/},
urldate = {2020-10-24}
}
@misc{ins-drift,
annote = {Accessed: 2020-09-10},
howpublished = {{\$}\backslash{\$}url{\{}https://www.skybrary.aero/index.php/Inertial{\_}Navigation{\_}System{\_}(INS){\}}},
title = {{INS drift}}
}
@inproceedings{brief,
abstract = {We propose to use binary strings as an efficient feature point descriptor, which we call BRIEF.We show that it is highly discriminative even when using relatively few bits and can be computed using simple intensity difference tests. Furthermore, the descriptor similarity can be evaluated using the Hamming distance, which is very efficient to compute, instead of the L 2 norm as is usually done. As a result, BRIEF is very fast both to build and to match. We compare it against SURF and U-SURF on standard benchmarks and show that it yields a similar or better recognition performance, while running in a fraction of the time required by either. {\textcopyright} 2010 Springer-Verlag.},
address = {Berlin, Heidelberg},
author = {Calonder, Michael and Lepetit, Vincent and Strecha, Christoph and Fua, Pascal},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-15561-1_56},
editor = {Daniilidis, Kostas and Maragos, Petros and Paragios, Nikos},
isbn = {364215560X},
issn = {16113349},
number = {PART 4},
pages = {778--792},
publisher = {Springer Berlin Heidelberg},
title = {{BRIEF: Binary robust independent elementary features}},
volume = {6314 LNCS},
year = {2010}
}
@article{hu,
abstract = {In this paper a theory of two-dimensional moment invariants for planar geometric figures is presented. A fundamental theorem is established to relate such moment invariants to the well-known algebraic invariants. Complete systems of moment invariants under translation, similitude and orthogonal transformations are derived. Some moment invariants under general two-dimensional linear transformations are also included. Both theoretical formulation and practical models of visual pattern recognition based upon these moment invariants are discussed. A simple simulation program together with its performance are also presented. It is shown that recognition of geometrical patterns and alphabetical characters independently of position, size and orientation can be accomplished. It is also indicated that generalization is possible to include invariance with parallel projection. {\textcopyright} 1962, IEEE. All right reserved.},
author = {Hu, Ming Kuei},
doi = {10.1109/TIT.1962.1057692},
issn = {21682712},
journal = {IRE Transactions on Information Theory},
number = {2},
pages = {179--187},
title = {{Visual Pattern Recognition by Moment Invariants}},
volume = {8},
year = {1962}
}
@article{mantelli,
abstract = {This paper presents a method for global localization and tracking of an Unmanned Aerial Vehicle (UAV) over satellite images. We propose a new measurement model based on a novel version of BRIEF descriptor and apply it in a Monte Carlo Localization system that estimates the UAV pose in 4 degrees of freedom. The model is used to compare images obtained from the UAV downward looking camera against patches of satellite images such as the ones available on Google™ Earth. The proposed method was validated using real flights sequences and has yield good results with different maps of the same region spawning many years and covering large areas.},
author = {Mantelli, Mathias and Pittol, Diego and Neuland, Renata and Ribacki, Arthur and Maffei, Renan and Jorge, Vitor and Prestes, Edson and Kolberg, Mariana},
doi = {10.1016/j.robot.2018.12.006},
issn = {09218890},
journal = {Robotics and Autonomous Systems},
keywords = {Localization,Satellite images,UAV,abBRIEF},
pages = {304--319},
title = {{A novel measurement model based on abBRIEF for global localization of a UAV over satellite images}},
url = {http://www.sciencedirect.com/science/article/pii/S092188901830438X},
volume = {112},
year = {2019}
}
@inproceedings{unet,
abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
address = {Cham},
archivePrefix = {arXiv},
arxivId = {1505.04597},
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-24574-4_28},
editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M and Frangi, Alejandro F},
eprint = {1505.04597},
isbn = {9783319245737},
issn = {16113349},
pages = {234--241},
publisher = {Springer International Publishing},
title = {{U-net: Convolutional networks for biomedical image segmentation}},
volume = {9351},
year = {2015}
}
@article{mclalg,
abstract = {Planning and navigation algorithms exploit statistics gleaned from uncertain, imperfect real-world environments to guide robots toward their goals and around obstacles.},
author = {Thrun, Sebastian},
doi = {10.1145/504729.504754},
isbn = {0262201623},
issn = {00010782},
journal = {Communications of the ACM},
number = {3},
pages = {52--57},
publisher = {The MIT Press},
title = {{Probabilistic robotics}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Probabilistic+Robotics+(Intelligent+Robotics+and+Autonomous+Agents){\#}0},
volume = {45},
year = {2002}
}
@article{Liu2017,
abstract = {Unsupervised image-to-image translation aims at learning a joint distribution of images in different domains by using images from the marginal distributions in individual domains. Since there exists an infinite set of joint distributions that can arrive the given marginal distributions, one could infer nothing about the joint distribution from the marginal distributions without additional assumptions. To address the problem, we make a shared-latent space assumption and propose an unsupervised image-to-image translation framework based on Coupled GANs. We compare the proposed framework with competing approaches and present high quality image translation results on various challenging unsupervised image translation tasks, including street scene image translation, animal image translation, and face image translation. We also apply the proposed framework to domain adaptation and achieve state-of-the-art performance on benchmark datasets. Code and additional results are available in https://github.com/mingyuliutw/unit.},
archivePrefix = {arXiv},
arxivId = {1703.00848},
author = {Liu, Ming Yu and Breuel, Thomas and Kautz, Jan},
eprint = {1703.00848},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
number = {Nips},
pages = {701--709},
title = {{Unsupervised image-to-image translation networks}},
volume = {2017-Decem},
year = {2017}
}
@article{wu,
abstract = {Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems—BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN's usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6{\%} lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation in COCO (https://github.com/facebookresearch/Detectron/blob/master/projects/GN), and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries.},
archivePrefix = {arXiv},
arxivId = {1803.08494},
author = {Wu, Yuxin and He, Kaiming},
doi = {10.1007/s11263-019-01198-w},
eprint = {1803.08494},
file = {:home/vegovs/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu, He - 2020 - Group Normalization.pdf:pdf},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Batch size,Image recognition,Normalization,Object detection},
month = {mar},
number = {3},
pages = {742--755},
publisher = {Springer},
title = {{Group Normalization}},
url = {https://github.com/facebookresearch/Detectron/},
volume = {128},
year = {2020}
}
@inproceedings{valada,
abstract = {Robust scene understanding of outdoor environments using passive optical sensors is a onerous and essential task for autonomous navigation. The problem is heavily characterized by changing environmental conditions throughout the day and across seasons. Robots should be equipped with models that are impervious to these factors in order to be operable and more importantly to ensure safety in the real-world. In this paper, we propose a novel semantic segmentation architecture and the convoluted mixture of deep experts (CMoDE) fusion technique that enables a multi-stream deep neural network to learn features from complementary modalities and spectra, each of which are specialized in a subset of the input space. Our model adaptively weighs class-specific features of expert networks based on the scene condition and further learns fused representations to yield robust segmentation. We present results from experimentation on three publicly available datasets that contain diverse conditions including rain, summer, winter, dusk, fall, night and sunset, and show that our approach exceeds the state-of-the-art. In addition, we evaluate the performance of autonomously traversing several kilometres of a forested environment using only the segmentation for perception.},
author = {Valada, Abhinav and Vertens, Johan and Dhall, Ankit and Burgard, Wolfram},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2017.7989540},
isbn = {9781509046331},
issn = {10504729},
pages = {4644--4651},
title = {{AdapNet: Adaptive semantic segmentation in adverse environmental conditions}},
year = {2017}
}
@inproceedings{viswanathan,
abstract = {This paper studies the problem of GPS-denied unmanned ground vehicle (UGV) localization by matching ground images to a satellite map. We examine the realistic, but particularly challenging problem of navigation in remote areas using maps that may correspond to a different season of the year. The problem is difficult due to the limited UGV sensor horizon, the drastic shift in perspective between ground and aerial views, the absence of discriminative features in the environment due to the remote location, and the high variation in appearance of the satellite map caused by the change in seasons. We present an approach to image matching using semantic information that is invariant to seasonal change. This semantics-based matching is incorporated into a particle filter framework and successful localization of the ground vehicle is demonstrated for satellite maps captured in summer, spring, and winter.},
author = {Viswanathan, Anirudh and Pires, Bernardo R. and Huber, Daniel},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2016.7487685},
isbn = {9781467380263},
issn = {10504729},
pages = {4815--4821},
title = {{Vision-based robot localization across seasons and in remote locations}},
volume = {2016-June},
year = {2016}
}
@article{carrol,
abstract = {During the course of its development for military use and more recent extension to many civilian uses, vulnerabilities of Global Navigation Satellite Systems (GNSS) - in the United States the Global Positioning System (GPS) - have become apparent. The vulnerabilities arise from natural, intentional, and unintentional sources. Increasing civilian and military reliance on GNSS brings with it a vital need to identify the critical vulnerabilities to civilian users, and to develop a plan to mitigate these vulnerabilities. This paper summarizes the findings of the U.S. Department of Transportation (DOT) vulnerability study that addresses these issues. The key findings are that satellite navigation users are vulnerable to several classes of disruption that affect all transportation modes and related infrastructure; but also that the vulnerabilities can be mitigated by awareness, planning, and using independent backup systems and/or alternate procedures in safety-critical applications. To gain the full benefits of GNSS, it will be necessary to analyse safety-of-life vulnerabilities in detail, and to determine the means and costs of reducing these risks to acceptable levels. The complete assessment report, of which this paper is a synopsis, was released to the public on September 10, 2001. Although the basic findings apply to all GNSS, the assessment focused on the GPS, in response to the enabling Presidential Decision Directive.},
annote = {Cited By :50},
author = {Carroll, James V.},
doi = {10.1017/S0373463303002273},
issn = {03734633},
journal = {Journal of Navigation},
keywords = {GNSS,GPS,Risk mitigation,Vulnerability},
number = {2},
pages = {185--193},
title = {{Vulnerability assessment of the U.S. transportation infrastructure that relies on the global positioning system}},
url = {www.scopus.com},
volume = {56},
year = {2003}
}
@article{Tønnessen,
author = {T{\o}nnessen, H{\aa}kon Haustreis and Skjeltorp, Ole Edvin and Bergsvik, Vegard and Mathias, {\O}vsteg{\aa}rd and Maffei, Renan},
number = {Dl},
title = {{Deep Learning-based Visual UAV Localization Robust to Environmental Changes}}
}
@misc{deepai_bs,
title = {{Batch Normalization Definition | DeepAI}},
url = {https://deepai.org/machine-learning-glossary-and-terms/batch-normalization},
urldate = {2020-10-23}
}
@misc{deepai_bp,
title = {{Backpropagation Definition | DeepAI}},
url = {https://deepai.org/machine-learning-glossary-and-terms/backpropagation},
urldate = {2020-10-25}
}
@misc{uav-usage,
abstract = {UAV uses and market trends},
annote = {Accessed: 2020-09-10},
author = {Murfin, Tony},
booktitle = {GPS World},
howpublished = {https://www.gpsworld.com/uav-report-growth-trends-opportunities-for-2019/},
title = {{UAV Report: Growth Trends {\&} Opportunities for 2019}},
url = {https://www.gpsworld.com/uav-report-growth-trends-opportunities-for-2019/},
year = {2018}
}
@inproceedings{he,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.90},
eprint = {1512.03385},
file = {:home/vegovs/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - 2016 - Deep residual learning for image recognition.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
month = {dec},
pages = {770--778},
publisher = {IEEE Computer Society},
title = {{Deep residual learning for image recognition}},
url = {http://image-net.org/challenges/LSVRC/2015/},
volume = {2016-Decem},
year = {2016}
}
@article{Information1962,
author = {Information, O N},
journal = {Pattern Recognition},
pages = {66--70},
title = {{by Moment}},
year = {1962}
}
@article{Arroyo2016,
abstract = {The extreme variability in the appearance of a place across the four seasons of the year is one of the most challenging problems in life-long visual topological localization for mobile robotic systems and intelligent vehicles. Traditional solutions to this problem are based on the description of images using hand-crafted features, which have been shown to offer moderate invariance against seasonal changes. In this paper, we present a new proposal focused on automatically learned descriptors, which are processed by means of a technique recently popularized in the computer vision community: Convolutional Neural Networks (CNNs). The novelty of our approach relies on fusing the image information from multiple convolutional layers at several levels and granularities. In addition, we compress the redundant data of CNN features into a tractable number of bits for efficient and robust place recognition. The final descriptor is reduced by applying simple compression and binarization techniques for fast matching using the Hamming distance. An exhaustive experimental evaluation confirms the improved performance of our proposal (CNN-VTL) with respect to state-of-the-art methods over varied long-term datasets recorded across seasons.},
author = {Arroyo, Roberto and Alcantarilla, Pablo F. and Bergasa, Luis M. and Romera, Eduardo},
doi = {10.1109/IROS.2016.7759685},
isbn = {9781509037629},
issn = {21530866},
journal = {IEEE International Conference on Intelligent Robots and Systems},
pages = {4656--4663},
title = {{Fusion and binarization of CNN features for robust topological localization across seasons}},
volume = {2016-Novem},
year = {2016}
}
@inproceedings{surber,
abstract = {Agile robots, such as small Unmanned Aerial Vehicles (UAVs) can have a great impact on the automation of tasks, such as industrial inspection and maintenance or crop monitoring and fertilization in agriculture. Their deploy-ability, however, relies on the UAV's ability to self-localize with precision and exhibit robustness to common sources of uncertainty in real missions. Here, we propose a new system using the UAV's onboard visual-inertial sensor suite to first build a Reference Map of the UAV's workspace during a piloted reconnaissance flight. In subsequent flights over this area, the proposed framework combines keyframe-based visual-inertial odometry with novel geometric image-based localization, to provide a real-time estimate of the UAV's pose with respect to the Reference Map paving the way towards completely automating repeated navigation in this workspace. The stability of the system is ensured by decoupling the local visual-inertial odometry from the global registration to the Reference Map, while GPS feeds are used as a weak prior for suggesting loop closures. The proposed framework is shown to outperform GPS localization significantly and diminishes drift effects via global image-based alignment for consistently robust performance.},
author = {Surber, Julian and Teixeira, Lucas and Chli, Margarita},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2017.7989745},
isbn = {9781509046331},
issn = {10504729},
pages = {6300--6306},
title = {{Robust visual-inertial localization with weak GPS priors for repetitive UAV flights}},
year = {2017}
}
@article{Masters2018,
abstract = {Modern deep neural network training is typically based on mini-batch stochastic gradient optimization. While the use of large mini-batches increases the available computational parallelism, small batch training has been shown to provide improved generalization performance and allows a significantly smaller memory footprint, which might also be exploited to improve machine throughput. In this paper, we review common assumptions on learning rate scaling and training duration, as a basis for an experimental comparison of test performance for different mini-batch sizes. We adopt a learning rate that corresponds to a constant average weight update per gradient calculation (i.e., per unit cost of computation), and point out that this results in a variance of the weight updates that increases linearly with the mini-batch size {\$}m{\$}. The collected experimental results for the CIFAR-10, CIFAR-100 and ImageNet datasets show that increasing the mini-batch size progressively reduces the range of learning rates that provide stable convergence and acceptable test performance. On the other hand, small mini-batch sizes provide more up-to-date gradient calculations, which yields more stable and reliable training. The best performance has been consistently obtained for mini-batch sizes between {\$}m = 2{\$} and {\$}m = 32{\$}, which contrasts with recent work advocating the use of mini-batch sizes in the thousands.},
archivePrefix = {arXiv},
arxivId = {1804.07612},
author = {Masters, Dominic and Luschi, Carlo},
eprint = {1804.07612},
title = {{Revisiting Small Batch Training for Deep Neural Networks}},
url = {https://www.graphcore.ai/posts/revisiting-small-batch-training-for-deep-neural-networks http://arxiv.org/abs/1804.07612},
year = {2018}
}
@misc{stanford,
title = {{CS231n Convolutional Neural Networks for Visual Recognition}},
url = {https://cs231n.github.io/},
urldate = {2020-10-23}
}
@misc{gps-accuracy,
abstract = {"GPS Accuracy". GPS.gov. GPS.gov. Retrieved 4 May 2015.},
annote = {Accessed: 2020-09-10},
author = {{US Air Force}},
howpublished = {https://www.gps.gov/systems/gps/performance/accuracy/},
title = {{GPS Accuracy}},
url = {https://www.gps.gov/systems/gps/performance/accuracy/},
year = {2017}
}
@article{fcn,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20{\%} relative improvement to 62.2{\%} mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038},
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
eprint = {1411.4038},
file = {:home/vegovs/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Long, Shelhamer, Darrell - 2014 - Fully Convolutional Networks for Semantic Segmentation.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Convolutional Networks,Deep Learning,Semantic Segmentation,Transfer Learning},
month = {nov},
number = {4},
pages = {640--651},
publisher = {IEEE Computer Society},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
url = {http://arxiv.org/abs/1411.4038},
volume = {39},
year = {2014}
}
@inproceedings{mcl,
abstract = {The use of maps allows mobile robots to navigate between known points in an environment. Using maps allows to calculate routes avoiding obstacles and not being stuck in dead ends. This paper shows how to integrate 3D perceptions on a map to obtain obstacle-free paths when obstacles are not at the level of 2D sensors, but elevated. Chairs and tables usually pose a problem when one can only see the legs with a 2D laser, although they present a high hurdle with a much larger area. This approach builds a static map starting from the construction plans of a building. A long-term map is started from the static map, and updated when adding and removing furniture, or when doors are opened or closed. A short-term map represents dynamic obstacles such as people. Obstacles are perceived by merging all available information, both 2D laser and RGB-D cameras, into a compact 3D probabilistic representation. This approach is appropriate for fast deployment and long-term operations in office or domestic environments, able to adapt to changes in the environment. This work is designed for domestic environments, and has been tested in the RoboCup@home competition, where robots must navigate in an environment that changes during the tests.},
address = {Cham},
author = {Gin{\'{e}}s, Jonathan and Mart{\'{i}}n, Francisco and Matell{\'{a}}n, Vicente and Lera, Francisco J. and Balsa, Jes{\'{u}}s},
booktitle = {Advances in Intelligent Systems and Computing},
doi = {10.1007/978-3-319-70836-2_24},
editor = {Ollero, Anibal and Sanfeliu, Alberto and Montano, Luis and Lau, Nuno and Cardeira, Carlos},
isbn = {9783319708355},
issn = {21945357},
keywords = {3D mapping,Long-term navigation,Mobile robot,Robocup},
pages = {283--294},
publisher = {Springer International Publishing},
title = {{3D Mapping for a Reliable Long-Term Navigation}},
volume = {694},
year = {2018}
}
@inproceedings{munet,
abstract = {In recent years, convolution neural network (CNN) has emerged as a dominant paradigm in machine learning for image processing application. In remote sensing, image segmentation is a very challenging task, and CNN has shown its worth over traditional image segmentation methods. U-Net structure is one of the simplified architectures used for image segmentation. However, it cannot extract promising spatial information from satellite data due to insufficient numbers of layers. In this study, a modified U-Net architecture has designed and proposed. This new architecture is deep enough to extract contextual information from satellite imagery. The study has formulated the downsampling part by introducing the DenseNet architecture, encourages the use of repetitive feature map, and reinforces the information propagation throughout the network. The long-range skip connections implemented between downsampling and upsampling. The quantitative comparison has performed using intersection over union (IOU) and overall accuracy (OA). The proposed architecture has shown 73.02{\%} and 96.02{\%} IOU and OA, respectively.},
address = {Singapore},
author = {Soni, Ashish and Koner, Radhakanta and Villuri, Vasanta Govind Kumar},
booktitle = {Advances in Intelligent Systems and Computing},
doi = {10.1007/978-981-15-2188-1_4},
editor = {Mandal, Jyotsna Kumar and Mukhopadhyay, Somnath},
isbn = {9789811521874},
issn = {21945365},
keywords = {Aerial imagery,Building detection,DenseNet architecture,Hybrid architecture,Skip connection},
pages = {47--59},
publisher = {Springer Singapore},
title = {{M-unet: Modified u-net segmentation framework with satellite imagery}},
volume = {1112},
year = {2020}
}
